<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>publications | muda lab</title> <meta name="author" content="muda lab"/> <meta name="description" content="ordered in reversed chronological order."/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://mudalab.github.io/publications/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">muda </span>lab</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description">ordered in reversed chronological order.</p> </header> <article> <p>For a longer list, please refer to the PI's <a href="https://scholar.google.com/citations?user=uH74dcgAAAAJ" target="_blank" rel="noopener noreferrer noopener noreferrer">Google Scholar page</a>.</p> <div class="publications"> <h2 class="year">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <img id="rotatedbb-wtbjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/rotatedbb-wtb.jpg"><div id="rotatedbb-wtbjpg-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('rotatedbb-wtbjpg-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="rotatedbb-wtbjpg-modal-img"> </div> <script>var modal=document.getElementById("rotatedbb-wtbjpg-modal"),img=document.getElementById("rotatedbb-wtbjpg"),modalImg=document.getElementById("rotatedbb-wtbjpg-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="gohar2024optimizing" class="col-sm-8"> <div class="title">Optimizing Wind Turbine Surface Defect Detection: A Rotated Bounding Box Approach</div> <div class="author"> Imad Gohar, Abderrahim Halimi, Yew Weng Kean, and John See</div> <div class="periodical"> <em>32nd European Conference on Signal Processing (to appear)</em>, 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://eurasip.org/Proceedings/Eusipco/Eusipco2024/pdfs/0000631.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" target="_blank" rel="noopener noreferrer"></a> </div> <div class="abstract hidden"> <p>Detecting surface defects on Wind Turbine Blades (WTBs) from remotely sensed images is a crucial step toward automated visual inspection. Typical object detection algorithms use standard bounding boxes to locate defects on WTBs. However, Oriented Bounding Boxes (OBBs) have been shown in cases of satellite imagery, to provide more precise localization of object regions and actual orientation. Existing WTB datasets do not depict defects using OBBs and this causes the lack of useful orientational information. In this paper, we consider OBBs for WTB surface defect detection through two publicly available datasets, introducing new annotations to the community. Baselines were constructed on state-of-the-art rotated object detectors, demonstrating considerable promise and known gaps that can be addressed in the future. We present a comprehensive analysis of their performances including ablation study and discussions on the importance of angular disparity between OBBs.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <img id="sfamnetpng" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/sfamnet.png"><div id="sfamnetpng-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('sfamnetpng-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="sfamnetpng-modal-img"> </div> <script>var modal=document.getElementById("sfamnetpng-modal"),img=document.getElementById("sfamnetpng"),modalImg=document.getElementById("sfamnetpng-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="liong2024sfamnet" class="col-sm-8"> <div class="title">SFAMNet: A Scene Flow Attention-based Micro-expression Network</div> <div class="author"> Gen-Bing Liong, Sze-Teng Liong, Chee Seng Chan, and John See</div> <div class="periodical"> <em>Neurocomputing</em>, 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.sciencedirect.com/science/article/pii/S0925231223011219" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://github.com/genbing99/SFAMNet" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right" data-doi="10.1016/j.neucom.2023.126998"></span> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.neucom.2023.126998" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1016/j.neucom.2023.126998" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" target="_blank" rel="noopener noreferrer"></a> </div> <div class="abstract hidden"> <p>Tremendous progress has been made in facial Micro-Expression (ME) spotting and recognition; however, most works have focused on either spotting or recognition tasks on the 2D videos. Until recently, the estimation of the 3D motion field (a.k.a scene flow) for the ME has only become possible after the release of the multi-modal ME dataset. In this paper, we propose the first Scene Flow Attention-based Micro-expression Network, namely SFAMNet. It takes the scene flow computed using the RGB-D flow algorithm as the input and predicts the spotting confidence score and emotion labels. Specifically, SFAMNet is an attention-based end-to-end multi-stream multi-task network devised to spot and recognize the ME. Besides that, we present a data augmentation strategy to alleviate the small sample size problem during network learning. Extensive experiments are performed on three tasks: (i) ME spotting; (ii) ME recognition; and (iii) ME analysis on the multi-modal CAS(ME)^3 dataset. Empirical results indicate that depth is vital in capturing the ME information and the effectiveness of the proposed approach. Our source code is publicly available at <a href="https://github.com/genbing99/SFAMNet" target="_blank" rel="noopener noreferrer">https://github.com/genbing99/SFAMNet</a>.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <img id="controllable-augsjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/controllable-augs.jpg"><div id="controllable-augsjpg-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('controllable-augsjpg-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="controllable-augsjpg-modal-img"> </div> <script>var modal=document.getElementById("controllable-augsjpg-modal"),img=document.getElementById("controllable-augsjpg"),modalImg=document.getElementById("controllable-augsjpg-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="qian2024controllable" class="col-sm-8"> <div class="title">Controllable augmentations for video representation learning</div> <div class="author"> Rui Qian, Weiyao Lin, John See, and Dian Li</div> <div class="periodical"> <em>Visual Intelligence</em>, 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://link.springer.com/article/10.1007/s44267-023-00034-7" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right" data-doi="10.1007/s44267-023-00034-7"></span> <span class="__dimensions_badge_embed__" data-doi="10.1007/s44267-023-00034-7" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1007/s44267-023-00034-7" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" target="_blank" rel="noopener noreferrer"></a> </div> <div class="abstract hidden"> <p>This paper focuses on self-supervised video representation learning. Most existing approaches follow the contrastive learning pipeline to construct positive and negative pairs by sampling different clips. However, this formulation tends to bias the static background and has difficulty establishing global temporal structures. The major reason is that the positive pairs, i.e., different clips sampled from the same video, have limited temporal receptive fields, and usually share similar backgrounds but differ in motions. To address these problems, we propose a framework to jointly utilize local clips and global videos to learn from detailed region-level correspondence as well as general long-term temporal relations. Based on a set of designed controllable augmentations, we implement accurate appearance and motion pattern alignment through soft spatio-temporal region contrast. Our formulation avoids the low-level redundancy shortcut with an adversarial mutual information minimization objective to improve the generalization ability. Moreover, we introduce local-global temporal order dependency to further bridge the gap between clip-level and video-level representations for robust temporal modeling. Extensive experiments demonstrate that our framework is superior on three video benchmarks in action recognition and video retrieval, and captures more accurate temporal dynamics.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <img id="scene-graphjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/scene-graph.jpg"><div id="scene-graphjpg-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('scene-graphjpg-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="scene-graphjpg-modal-img"> </div> <script>var modal=document.getElementById("scene-graphjpg-modal"),img=document.getElementById("scene-graphjpg"),modalImg=document.getElementById("scene-graphjpg-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="lin2024scene" class="col-sm-8"> <div class="title">Scene Graph Lossless Compression with Adaptive Prediction for Objects and Relations</div> <div class="author"> Weiyao Lin, Yufeng Zhang, Wenrui Dai, Huabin Liu, John See, and Hongkai Xiong</div> <div class="periodical"> <em>ACM Transactions on Multimedia Computing, Communications and Applications</em>, 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/full/10.1145/3649503" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right" data-doi="10.1145/3649503"></span> <span class="__dimensions_badge_embed__" data-doi="10.1145/3649503" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1145/3649503" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" target="_blank" rel="noopener noreferrer"></a> </div> <div class="abstract hidden"> <p>The scene graph is a novel data structure describing objects and their pairwise relationship within image scenes. As the size of scene graphs in vision and multimedia applications increases, the need for lossless storage and transmission of such data becomes more critical. However, the compression of scene graphs is less studied because of the complicated data structures involved and complex distributions. Existing solutions usually involve general-purpose compressors or graph structure compression methods, which are weak at reducing the redundancy in scene graph data. This article introduces a novel lossless compression framework with adaptive predictors for the joint compression of objects and relations in scene graph data. The proposed framework comprises a unified prior extractor and specialized element predictors to adapt to different data elements. Furthermore, to exploit the context information within and between graph elements, Graph Context Convolution is proposed to support different graph context modeling schemes for different graph elements. Finally, an overarching framework incorporates the learned distribution model to predict numerical data under complicated conditional constraints. Experiments conducted on labeled or generated scene graphs demonstrate the effectiveness of the proposed framework for scene graph lossless compression.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <img id="unified-compressionjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/unified-compression.jpg"><div id="unified-compressionjpg-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('unified-compressionjpg-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="unified-compressionjpg-modal-img"> </div> <script>var modal=document.getElementById("unified-compressionjpg-modal"),img=document.getElementById("unified-compressionjpg"),modalImg=document.getElementById("unified-compressionjpg-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="liu2024unified" class="col-sm-8"> <div class="title">A Unified Framework for Jointly Compressing Visual and Semantic Data</div> <div class="author"> Shizhan Liu, Weiyao Lin, Yihang Chen, Yufeng Zhang, Wenrui Dai, John See, and Hong-Kai Xiong</div> <div class="periodical"> <em>ACM Transactions on Multimedia Computing, Communications and Applications</em>, 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/full/10.1145/3654800" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right" data-doi="10.1145/3654800"></span> <span class="__dimensions_badge_embed__" data-doi="10.1145/3654800" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1145/3654800" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" target="_blank" rel="noopener noreferrer"></a> </div> <div class="abstract hidden"> <p>The rapid advancement of multimedia and imaging technologies has resulted in increasingly diverse visual and semantic data. A large range of applications such as remote-assisted driving requires the amalgamated storage and transmission of various visual and semantic data. However, existing works suffer from the limitation of insufficiently exploiting the redundancy between different types of data. In this article, we propose a unified framework to jointly compress a diverse spectrum of visual and semantic data, including images, point clouds, segmentation maps, object attributes, and relations. We develop a unifying process that embeds the representations of these data into a joint embedding graph according to their categories, which enables flexible handling of joint compression tasks for various visual and semantic data. To fully leverage the redundancy between different data types, we further introduce an embedding-based adaptive joint encoding process and a Semantic Adaptation Module to efficiently encode diverse data based on the learned embeddings in the joint embedding graph. Experiments on the Cityscapes, MSCOCO, and KITTI datasets demonstrate the superiority of our framework, highlighting promising steps toward scalable multimedia processing.</p> </div> </div> </div> </li> </ol> <h2 class="year">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <img id="spot-then-recognizejpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/spot-then-recognize.jpg"><div id="spot-then-recognizejpg-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('spot-then-recognizejpg-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="spot-then-recognizejpg-modal-img"> </div> <script>var modal=document.getElementById("spot-then-recognizejpg-modal"),img=document.getElementById("spot-then-recognizejpg"),modalImg=document.getElementById("spot-then-recognizejpg-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="liong2023spot" class="col-sm-8"> <div class="title">Spot-then-recognize: A micro-expression analysis network for seamless evaluation of long videos</div> <div class="author"> Gen-Bing Liong, John See, and Chee-Seng Chan</div> <div class="periodical"> <em>Signal Processing: Image Communication</em>, 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0923596522001540" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://github.com/genbing99/MEAN_Spot-then-recognize" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right" data-doi="10.1016/j.image.2022.116875"></span> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.image.2022.116875" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1016/j.image.2022.116875" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" target="_blank" rel="noopener noreferrer"></a> </div> <div class="abstract hidden"> <p>Facial Micro-Expressions (MEs) reveal a person’s hidden emotions in high stake situations within a fraction of a second and at a low intensity. The broad range of potential real-world applications that can be applied has drawn considerable attention from researchers in recent years. However, both spotting and recognition tasks are often treated separately. In this paper, we present Micro-Expression Analysis Network (MEAN), a shallow multi-stream multi-output network architecture comprising of task-specific (spotting and recognition) networks that is designed to effectively learn a meaningful representation from both ME class labels and location-wise pseudo-labels. Notably, this is the first known work that addresses ME analysis on long videos using a deep learning approach, whereby ME spotting and recognition are performed sequentially in a two-step procedure: first spotting the ME intervals using the spotting network, and proceeding to predict their emotion classes using the recognition network. We report extensive benchmark results on the ME analysis task on both short video datasets (CASME II, SMIC-E-HS, SMIC-E-VIS, and SMIC-E-NIR), and long video datasets (CAS(ME)2 and SAMMLV); the latter in particular demonstrates the capability of the proposed approach under unconstrained settings. Besides the standard measures, we promote the usage of fairer metrics in evaluating the performance of a complete ME analysis system. We also provide visual explanations of where the network is “looking” and showcasing the effectiveness of inductive transfer applied during network training. An analysis is performed on the in-the-wild dataset (MEVIEW) to open up further research into real-world scenarios.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <img id="emostorypng" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/emostory.png"><div id="emostorypng-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('emostorypng-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="emostorypng-modal-img"> </div> <script>var modal=document.getElementById("emostorypng-modal"),img=document.getElementById("emostorypng"),modalImg=document.getElementById("emostorypng-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="too2023emostory" class="col-sm-8"> <div class="title">EmoStory: Emotion Prediction and Mapping in Narrative Stories</div> <div class="author"> Seng Wei Too, John See, Albert Quek, and Hui Ngo Goh</div> <div class="periodical"> <em>International Journal on Informatics Visualization</em>, 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://joiv.org/index.php/joiv/article/view/2335" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://researchportal.hw.ac.uk/files/139343772/2335-5649-1-PB.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right" data-doi="10.30630/joiv.7.3-2.2335"></span> <span class="__dimensions_badge_embed__" data-doi="10.30630/joiv.7.3-2.2335" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.30630/joiv.7.3-2.2335" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" target="_blank" rel="noopener noreferrer"></a> </div> <div class="abstract hidden"> <p>A well-designed story is built upon a sequence of plots and events. Each event has its purpose in piquing the audience’s interest in the plot; thus, understanding the flow of emotions within the story is vital to its success. A story is usually built up through dramatic changes in emotion and mood to create resonance with the audience. The lack of research in this understudied field warrants exploring several aspects of the emotional analysis of stories. In this paper, we propose an encoder-decoder framework to perform sentence-level emotion recognition of narrative stories on both dimensional and categorical aspects, achieving MAE=0.0846 and 54% accuracy (8-class), respectively, on the EmoTales dataset and a reasonably good level of generalization to an untrained dataset. The first use of attention and multi-head attention mechanisms for emotion representation mapping (ERM) yields state-of-the-art performance in certain settings. We further present the preliminary idea of EmoStory, a concept that seamlessly predicts both dimensional and categorical space in an efficient manner, made possible with ERM. This methodology is useful in only one of the two aspects is available. In the future, these techniques could be extended to model the personality or emotional state of characters in stories, which could benefit the affective assessment of experiences and the creation of emotive avatars and virtual worlds.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <img id="moiredetpng" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/moiredet.png"><div id="moiredetpng-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('moiredetpng-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="moiredetpng-modal-img"> </div> <script>var modal=document.getElementById("moiredetpng-modal"),img=document.getElementById("moiredetpng"),modalImg=document.getElementById("moiredetpng-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="yang2023doing" class="col-sm-8"> <div class="title">Doing More With Moiré Pattern Detection in Digital Photos</div> <div class="author"> Cong Yang, Zhenyu Yang, Yan Ke, Tao Chen, Marcin Grzegorzek, and John See</div> <div class="periodical"> <em>IEEE Transactions on Image Processing</em>, 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/document/10006755" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://researchportal.hw.ac.uk/files/83582564/Doing_More_With_Moir_Pattern_Detection_in_Digital_Photos.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/cong-yang/MoireDet" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right" data-doi="10.1109/TIP.2022.3232232"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/TIP.2022.3232232" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1109/TIP.2022.3232232" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" target="_blank" rel="noopener noreferrer"></a> </div> <div class="abstract hidden"> <p>Detecting moiré patterns in digital photographs is meaningful as it provides priors towards image quality evaluation and demoiréing tasks. In this paper, we present a simple yet efficient framework to extract moiré edge maps from images with moiré patterns. The framework includes a strategy for training triplet (natural image, moiré layer, and their synthetic mixture) generation, and a Moiré Pattern Detection Neural Network (MoireDet) for moiré edge map estimation. This strategy ensures consistent pixel-level alignments during training, accommodating characteristics of a diverse set of camera-captured screen images and real-world moiré patterns from natural images. The design of three encoders in MoireDet exploits both high-level contextual and low-level structural features of various moiré patterns. Through comprehensive experiments, we demonstrate the advantages of MoireDet: better identification precision of moiré images on two datasets, and a marked improvement over state-of-the-art demoiréing methods.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <img id="slice-wtbjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/slice-wtb.jpg"><div id="slice-wtbjpg-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('slice-wtbjpg-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="slice-wtbjpg-modal-img"> </div> <script>var modal=document.getElementById("slice-wtbjpg-modal"),img=document.getElementById("slice-wtbjpg"),modalImg=document.getElementById("slice-wtbjpg-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="gohar2023slice" class="col-sm-8"> <div class="title">Slice-Aided Defect Detection in Ultra High-Resolution Wind Turbine Blade Images</div> <div class="author"> Imad Gohar, Abderrahim Halimi, John See, Weng Kean Yew, and Cong Yang</div> <div class="periodical"> <em>Machines</em>, 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.mdpi.com/2075-1702/11/10/953" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://www.mdpi.com/2075-1702/11/10/953/pdf?version=1697117170" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/imadgohar/DTU-annotations" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right" data-doi="10.3390/machines11100953"></span> <span class="__dimensions_badge_embed__" data-doi="10.3390/machines11100953" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.3390/machines11100953" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" target="_blank" rel="noopener noreferrer"></a> </div> <div class="abstract hidden"> <p>The processing of aerial images taken by drones is a challenging task due to their high resolution and the presence of small objects. The scale of the objects varies diversely depending on the position of the drone, which can result in loss of information or increased difficulty in detecting small objects. To address this issue, images are either randomly cropped or divided into small patches before training and inference. This paper proposes a defect detection framework that harnesses the advantages of slice-aided inference for small and medium-size damage on the surface of wind turbine blades. This framework enables the comparison of different slicing strategies, including a conventional patch division strategy and a more recent slice-aided hyper-inference, on several state-of-the-art deep neural network baselines for the detection of surface defects in wind turbine blade images. Our experiments provide extensive empirical results, highlighting the benefits of using the slice-aided strategy and the significant improvements made by these networks on an ultra high-resolution drone image dataset.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <img id="skeletonGTpng" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/skeletonGT.png"><div id="skeletonGTpng-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('skeletonGTpng-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="skeletonGTpng-modal-img"> </div> <script>var modal=document.getElementById("skeletonGTpng-modal"),img=document.getElementById("skeletonGTpng"),modalImg=document.getElementById("skeletonGTpng-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="yang2023skeleton" class="col-sm-8"> <div class="title">Skeleton Ground Truth Extraction: Methodology, Annotation Tool and Benchmarks</div> <div class="author"> Cong Yang, Bipin Indurkhya, John See, Bo Gao, Yan Ke, Zeyd Boukhers, Zhenyu Yang, and Marcin Grzegorzek</div> <div class="periodical"> <em>International Journal of Computer Vision</em>, 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2310.06437" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://link.springer.com/article/10.1007/s11263-023-01926-3" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://researchportal.hw.ac.uk/files/104065760/s11263-023-01926-3.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/cong-yang/skeview" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right" data-doi="10.1007/s11263-023-01926-3"></span> <span class="__dimensions_badge_embed__" data-doi="10.1007/s11263-023-01926-3" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1007/s11263-023-01926-3" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" target="_blank" rel="noopener noreferrer"></a> </div> <div class="abstract hidden"> <p>Skeleton Ground Truth (GT) is critical to the success of supervised skeleton extraction methods, especially with the popularity of deep learning techniques. Furthermore, we see skeleton GTs used not only for training skeleton detectors with Convolutional Neural Networks (CNN), but also for evaluating skeleton-related pruning and matching algorithms. However, most existing shape and image datasets suffer from the lack of skeleton GT and inconsistency of GT standards. As a result, it is difficult to evaluate and reproduce CNN-based skeleton detectors and algorithms on a fair basis. In this paper, we present a heuristic strategy for object skeleton GT extraction in binary shapes and natural images. Our strategy is built on an extended theory of diagnosticity hypothesis, which enables encoding human-in-the-loop GT extraction based on clues from the target’s context, simplicity, and completeness. Using this strategy, we developed a tool, SkeView, to generate skeleton GT of 17 existing shape and image datasets. The GTs are then structurally evaluated with representative methods to build viable baselines for fair comparisons. Experiments demonstrate that GTs generated by our strategy yield promising quality with respect to standard consistency, and also provide a balance between simplicity and completeness.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <img id="ernetpng" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/ernet.png"><div id="ernetpng-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('ernetpng-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="ernetpng-modal-img"> </div> <script>var modal=document.getElementById("ernetpng-modal"),img=document.getElementById("ernetpng"),modalImg=document.getElementById("ernetpng-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="lim2023ernet" class="col-sm-8"> <div class="title">ERNet: An Efficient and Reliable Human-Object Interaction Detection Network</div> <div class="author"> JunYi Lim, Vishnu Monn Baskaran, Joanne Mun-Yee Lim, KokSheik Wong, John See, and Massimo Tistarelli</div> <div class="periodical"> <em>IEEE Transactions on Image Processing</em>, 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://github.com/Monash-CyPhi-AI-Research-Lab/ernet" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right" data-doi="10.1109/TIP.2022.3231528"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/TIP.2022.3231528" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1109/TIP.2022.3231528" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" target="_blank" rel="noopener noreferrer"></a> </div> <div class="abstract hidden"> <p>Human-Object Interaction (HOI) detection recognizes how persons interact with objects, which is advantageous in autonomous systems such as self-driving vehicles and collaborative robots. However, current HOI detectors are often plagued by model inefficiency and unreliability when making a prediction, which consequently limits its potential for real-world scenarios. In this paper, we address these challenges by proposing ERNet, an end-to-end trainable convolutional-transformer network for HOI detection. The proposed model employs an efficient multi-scale deformable attention to effectively capture vital HOI features. We also put forward a novel detection attention module to adaptively generate semantically rich instance and interaction tokens. These tokens undergo pre-emptive detections to produce initial region and vector proposals that also serve as queries which enhances the feature refinement process in the transformer decoders. Several impactful enhancements are also applied to improve the HOI representation learning. Additionally, we utilize a predictive uncertainty estimation framework in the instance and interaction classification heads to quantify the uncertainty behind each prediction. By doing so, we can accurately and reliably predict HOIs even under challenging scenarios. Experiment results on the HICO-Det, V-COCO, and HOI-A datasets demonstrate that the proposed model achieves state-of-the-art performance in detection accuracy and training efficiency. Codes are publicly available at <a href="https://github.com/Monash-CyPhi-AI-Research-Lab/ernet" target="_blank" rel="noopener noreferrer">https://github.com/Monash-CyPhi-AI-Research-Lab/ernet</a>.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <img id="megc2023png" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/megc2023.png"><div id="megc2023png-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('megc2023png-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="megc2023png-modal-img"> </div> <script>var modal=document.getElementById("megc2023png-modal"),img=document.getElementById("megc2023png"),modalImg=document.getElementById("megc2023png-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="davison2023megc2023" class="col-sm-8"> <div class="title">MEGC2023: ACM Multimedia 2023 ME Grand Challenge</div> <div class="author"> Adrian K Davison, Jingting Li, Moi Hoon Yap, John See, Wen-Huang Cheng, Xiaobai Li, Xiaopeng Hong, and Su-Jing Wang</div> <div class="periodical"> <em>Proceedings of the 31st ACM International Conference on Multimedia</em>, 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3581783.3612833" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://megc2023.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right" data-doi="10.1145/3581783.3612833"></span> <span class="__dimensions_badge_embed__" data-doi="10.1145/3581783.3612833" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1145/3581783.3612833" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" target="_blank" rel="noopener noreferrer"></a> </div> <div class="abstract hidden"> <p>Facial micro-expressions (MEs) are involuntary movements of the face that occur spontaneously when a person experiences an emotion but attempts to suppress or repress the facial expression, typically found in a high-stakes environment. Unfortunately, the small sample problem severely limits the automation of ME analysis. Furthermore, due to the weak and transient nature of MEs, it is difficult for models to distinguish it from other types of facial actions. Therefore, ME in long videos is a challenging task, and the current performance cannot meet the practical application requirements. Addressing these issues, this challenge focuses on ME and the macro-expression (MaE) spotting task. This year, in order to evaluate algorithms’ performance more fairly, based on CAS(ME)2, SAMM Long Videos, SMIC-E-long, CAS(ME)3 and 4DME, we build an unseen cross-cultural long-video test set. All participating algorithms are required to run on this test set and submit their results on a leaderboard with a baseline result.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <img id="blade-stitchingjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/blade-stitching.jpg"><div id="blade-stitchingjpg-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('blade-stitchingjpg-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="blade-stitchingjpg-modal-img"> </div> <script>var modal=document.getElementById("blade-stitchingjpg-modal"),img=document.getElementById("blade-stitchingjpg"),modalImg=document.getElementById("blade-stitchingjpg-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="yang2023towards" class="col-sm-8"> <div class="title">Towards accurate image stitching for drone-based wind turbine blade inspection</div> <div class="author"> Cong Yang, Xun Liu, Hua Zhou, Yan Ke, and John See</div> <div class="periodical"> <em>Renewable Energy</em>, 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.sciencedirect.com/science/article/pii/S0960148122018481" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://github.com/cong-yang/Blade30" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Dataset</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right" data-doi="10.1016/j.renene.2022.12.063"></span> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.renene.2022.12.063" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1016/j.renene.2022.12.063" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" target="_blank" rel="noopener noreferrer"></a> </div> <div class="abstract hidden"> <p>Accurate image stitching is crucial to wind turbine blade visualization and defect analysis. It is inevitable that drone-captured images for blade inspection are high resolution and heavily overlapped. This also necessitates the stitching-based deduplication process on detected defects. However, the stitching task suffers from texture-poor blade surfaces, unstable drone pose (especially off-shore), and the lack of public blade datasets that cater to real-world challenges. In this paper, we present a simple yet efficient algorithm for robust and accurate blade image stitching. To promote further research, we also introduce a new dataset, Blade30, which contains 1,302 real drone-captured images covering 30 full blades captured under various conditions (both on- and off-shore), accompanied by a rich set of annotations such as defects and contaminations, etc. The proposed stitching algorithm generates the initial blade panorama based on blade edges and drone-blade distances at the coarse-grained level, followed by fine-grained adjustments optimized by regression-based texture and shape losses. Our method also fully utilizes the properties of blade images and prior information of the drone. Experiments report promising accuracy in blade stitching and defect deduplication tasks in the vision-based wind turbine blade inspection scenario, surpassing the performance of existing methods.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <img id="modality-ad-insertionjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/modality-ad-insertion.jpg"><div id="modality-ad-insertionjpg-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('modality-ad-insertionjpg-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="modality-ad-insertionjpg-modal-img"> </div> <script>var modal=document.getElementById("modality-ad-insertionjpg-modal"),img=document.getElementById("modality-ad-insertionjpg"),modalImg=document.getElementById("modality-ad-insertionjpg-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="chong2023modality" class="col-sm-8"> <div class="title">What Modality Matters? Exploiting Highly Relevant Features for Video Advertisement Insertion</div> <div class="author"> Onn Keat Chong, Hui-Ngo Goh, and John See</div> <div class="periodical"> <em>IEEE International Conference on Image Processing (ICIP)</em>, 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/abstract/document/10222693" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right" data-doi="10.1109/ICIP49359.2023.10222693"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/ICIP49359.2023.10222693" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1109/ICIP49359.2023.10222693" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" target="_blank" rel="noopener noreferrer"></a> </div> <div class="abstract hidden"> <p>Video advertising is a thriving industry that has recently turned its attention to the use of intelligent algorithms for automating tasks. In advertisement insertion, the integration of contextual relevance is essential in influencing the viewer’s experience. Despite the wide spectrum of audio-visual semantic modalities available, there is a lack of research that analyzes their individual and complementary strengths in a systematic manner. In this paper, we propose an ad-insertion framework that maximizes the contextual relevance between advertisement and content video by employing high-level multi-modal semantic features. Prediction vectors are derived via clip-level and image-level extractors, which are then matched accordingly to yield relevance scores. We also established a new user study methodology that produces gold standard annotations based on multiple expert selections. By comprehensive human-centered approaches and analysis, we demonstrate that automatic ad-insertion can be improved by exploiting effective combinations of semantic modalities.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <img id="context-awarejpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/context-aware.jpg"><div id="context-awarejpg-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('context-awarejpg-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="context-awarejpg-modal-img"> </div> <script>var modal=document.getElementById("context-awarejpg-modal"),img=document.getElementById("context-awarejpg"),modalImg=document.getElementById("context-awarejpg-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="nagappan2023context" class="col-sm-8"> <div class="title">Context-Aware Multi-Stream Networks for Dimensional Emotion Prediction in Images</div> <div class="author"> Sidharrth Nagappan, Jia Qi Tan, Lai-Kuan Wong, and John See</div> <div class="periodical"> <em>IEEE International Conference on Image Processing (ICIP)</em>, 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/abstract/document/10221960" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right" data-doi="10.1109/ICIP49359.2023.10221960"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/ICIP49359.2023.10221960" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1109/ICIP49359.2023.10221960" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" target="_blank" rel="noopener noreferrer"></a> </div> <div class="abstract hidden"> <p>Teaching machines to comprehend the nuances of emotion from photographs is a particularly challenging task. Emotion perception— naturally a subjective problem, is often simplified for computational purposes into categorical states or valence-arousal dimensional space, the latter being a lesser-explored problem in the literature. This paper proposes a multi-stream context-aware neural network model for dimensional emotion prediction in images. Models were trained using a set of object and scene data along with deep features for valence, arousal, and dominance estimation. Experimental evaluation on a large-scale image emotion dataset demonstrates the viability of our proposed approach. Our analysis postulates that the understanding of the depicted object in an image is vital for successful predictions whilst relying on scene information can lead to somewhat confounding effects.</p> </div> </div> </div> </li> </ol> <h2 class="year">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <img id="fatigueviewpng" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/fatigueview.png"><div id="fatigueviewpng-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('fatigueviewpng-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="fatigueviewpng-modal-img"> </div> <script>var modal=document.getElementById("fatigueviewpng-modal"),img=document.getElementById("fatigueviewpng"),modalImg=document.getElementById("fatigueviewpng-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="yang2022fatigueview" class="col-sm-8"> <div class="title">FatigueView: A Multi-Camera Video Dataset for Vision-Based Drowsiness Detection</div> <div class="author"> Cong Yang, Zhenyu Yang, Weiyu Li, and John See</div> <div class="periodical"> <em>IEEE Transactions on Intelligent Transportation Systems</em>, 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/abstract/document/9931532" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://pure.hw.ac.uk/ws/files/67344144/FatigueView_A_Multi_Camera_Video_Dataset_for_Vision_Based_Drowsiness_Detection.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/FatigueView/fatigueview" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://fatigueview.github.io" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right" data-doi="10.1109/TITS.2022.3216017"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/TITS.2022.3216017" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1109/TITS.2022.3216017" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" target="_blank" rel="noopener noreferrer"></a> </div> <div class="abstract hidden"> <p>Although vision-based drowsiness detection approaches have achieved great success on empirically organized datasets, it remains far from being satisfactory for deployment in practice. One crucial issue lies in the scarcity and lack of datasets that represent the actual challenges in real-world applications, e.g. tremendous variation and aggregation of visual signs, challenges brought on by different camera positions and camera types. To promote research in this field, we introduce a new large-scale dataset, FatigueView, that is collected by both RGB and infrared (IR) cameras from five different positions. It contains real sleepy driving videos and various visual signs of drowsiness from subtle to obvious, e.g. with 17,403 different yawning sets totaling more than 124 million frames, far more than recent actively used datasets. We also provide hierarchical annotations for each video, ranging from spatial face landmarks and visual signs to temporal drowsiness locations and levels to meet different research requirements. We structurally evaluate representative methods to build viable baselines. With FatigueView, we would like to encourage the community to adapt computer vision models to address practical real-world concerns, particularly the challenges posed by this dataset.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <img id="badmintondbjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/badmintondb.jpg"><div id="badmintondbjpg-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('badmintondbjpg-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="badmintondbjpg-modal-img"> </div> <script>var modal=document.getElementById("badmintondbjpg-modal"),img=document.getElementById("badmintondbjpg"),modalImg=document.getElementById("badmintondbjpg-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="ban2022badmintondb" class="col-sm-8"> <div class="title">Badmintondb: A badminton dataset for player-specific match analysis and prediction</div> <div class="author"> Kar-Weng Ban, John See, Junaidi Abdullah, and Yuen Peng Loh</div> <div class="periodical"> <em>Proceedings of the 5th International ACM Workshop on Multimedia Content Analysis in Sports</em>, 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3552437.3555696" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://github.com/kwban/badminton-db" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Dataset</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right" data-doi="10.1145/3552437.3555696"></span> <span class="__dimensions_badge_embed__" data-doi="10.1145/3552437.3555696" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1145/3552437.3555696" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" target="_blank" rel="noopener noreferrer"></a> </div> <div class="abstract hidden"> <p>This paper introduces BadmintonDB, a new badminton dataset for training models for player-specific match analysis and prediction tasks, which are interesting challenges. The dataset features rally, strokes, and outcome annotations of 9 real-world badminton matches between two top players. We discussed our methodologies and processes behind selecting and annotating the matches. We also proposed player-independent and player-dependent Naive Bayes baselines for rally outcome prediction. The paper concludes with the analysis performed on the experiments to study the effects of player-dependent model on the prediction performances. We released our dataset at <a href="https://github.com/kwban/badminton-db" target="_blank" rel="noopener noreferrer">https://github.com/kwban/badminton-db</a>.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <img id="pseudo-labelingjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/pseudo-labeling.jpg"><div id="pseudo-labelingjpg-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('pseudo-labelingjpg-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="pseudo-labelingjpg-modal-img"> </div> <script>var modal=document.getElementById("pseudo-labelingjpg-modal"),img=document.getElementById("pseudo-labelingjpg"),modalImg=document.getElementById("pseudo-labelingjpg-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="liong2022mtsn" class="col-sm-8"> <div class="title">Mtsn: A multi-temporal stream network for spotting facial macro-and micro-expression with hard and soft pseudo-labels</div> <div class="author"> Gen Bing Liong, Sze-Teng Liong, John See, and Chee-Seng Chan</div> <div class="periodical"> <em>Proceedings of the 2nd Workshop on Facial Micro-Expression: Advanced Techniques for Multi-Modal Facial Expression Analysis</em>, 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3552465.3555040" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://github.com/genbing99/MTSN-Spot-ME-MaE" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right" data-doi="10.1145/3552465.3555040"></span> <span class="__dimensions_badge_embed__" data-doi="10.1145/3552465.3555040" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1145/3552465.3555040" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" target="_blank" rel="noopener noreferrer"></a> </div> <div class="abstract hidden"> <p>This paper considers the challenge of spotting facial macro- and micro-expression from long videos. We propose the multi-temporal stream network (MTSN) model that takes two distinct inputs by considering the different temporal information in the facial movement. We also introduce a hard and soft pseudo-labeling technique to enable the network to distinguish expression frames from non-expression frames via the learning of salient features in the expression peak frame. Consequently, we demonstrate how a single output from the MTSN model can be post-processed to predict both macro- and micro-expression intervals. Our results outperform the MEGC 2022 baseline method significantly by achieving an overall F1-score of 0.2586 and also did remarkably well on the MEGC 2021 benchmark with an overall F1-score of 0.3620 and 0.2867 on CAS(ME)2 and SAMM Long Videos, respectively.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <img id="blumnetjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/blumnet.jpg"><div id="blumnetjpg-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('blumnetjpg-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="blumnetjpg-modal-img"> </div> <script>var modal=document.getElementById("blumnetjpg-modal"),img=document.getElementById("blumnetjpg"),modalImg=document.getElementById("blumnetjpg-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="zhang2022blumnet" class="col-sm-8"> <div class="title">BlumNet: Graph component detection for object skeleton extraction</div> <div class="author"> Yulu Zhang, Liang Sang, Marcin Grzegorzek, John See, and Cong Yang</div> <div class="periodical"> <em>Proceedings of the 30th ACM International Conference on Multimedia</em>, 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3547816" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right" data-doi="10.1145/3503161.3547816"></span> <span class="__dimensions_badge_embed__" data-doi="10.1145/3503161.3547816" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1145/3503161.3547816" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" target="_blank" rel="noopener noreferrer"></a> </div> <div class="abstract hidden"> <p>In this paper, we present a simple yet efficient framework, BlumNet, for extracting object skeletons in natural images and binary shapes. With the need for highly reliable skeletons in various multimedia applications, the proposed BlumNet is distinguished in three aspects: (1) The inception of graph decomposition and reconstruction strategies further simplifies the skeleton extraction task into a graph component detection problem, which significantly improves the accuracy and robustness of extracted skeletons. (2) The intuitive representation of each skeleton branch with multiple structured and overlapping line segments can effectively prevent the skeleton branch vanishing problem. (3) In comparison to traditional skeleton heatmaps, our approach directly outputs skeleton graphs, which is more feasible for real-world applications. Through comprehensive experiments, we demonstrate the advantages of BlumNet: significantly higher accuracy than the state-of-the-art AdaLSN (0.826 vs. 0.786) on the SK1491 dataset, a marked improvement in robustness on mixed object deformations, and also a state-of-the-art performance on binary shape datasets (e.g. 0.893 on the MPEG7 dataset).</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <img id="task-adaptivejpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/task-adaptive.jpg"><div id="task-adaptivejpg-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('task-adaptivejpg-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="task-adaptivejpg-modal-img"> </div> <script>var modal=document.getElementById("task-adaptivejpg-modal"),img=document.getElementById("task-adaptivejpg"),modalImg=document.getElementById("task-adaptivejpg-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="liu2022task" class="col-sm-8"> <div class="title">Task-adaptive spatial-temporal video sampler for few-shot action recognition</div> <div class="author"> Huabin Liu, Weixian Lv, John See, and Weiyao Lin</div> <div class="periodical"> <em>Proceedings of the 30th ACM International Conference on Multimedia</em>, 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3547938" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://github.com/R00Kie-Liu/Sampler" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right" data-doi="10.1145/3503161.354793"></span> <span class="__dimensions_badge_embed__" data-doi="10.1145/3503161.354793" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1145/3503161.354793" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" target="_blank" rel="noopener noreferrer"></a> </div> <div class="abstract hidden"> <p>A primary challenge faced in few-shot action recognition is inadequate video data for training. To address this issue, current methods in this field mainly focus on devising algorithms at the feature level while little attention is paid to processing input video data. Moreover, existing frame sampling strategies may omit critical action information in temporal and spatial dimensions, which further impacts video utilization efficiency. In this paper, we propose a novel video frame sampler for few-shot action recognition to address this issue, where task-specific spatial-temporal frame sampling is achieved via a temporal selector (TS) and a spatial amplifier (SA). Specifically, our sampler first scans the whole video at a small computational cost to obtain a global perception of video frames. The TS plays its role in selecting top-T frames that contribute most significantly and subsequently. The SA emphasizes the discriminative information of each frame by amplifying critical regions with the guidance of saliency maps. We further adopt task-adaptive learning to dynamically adjust the sampling strategy according to the episode task at hand. Both the implementations of TS and SA are differentiable for end-to-end optimization, facilitating seamless integration of our proposed sampler with most few-shot action recognition methods. Extensive experiments show a significant boost in the performances on various benchmarks including long-term videos.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <img id="ta2njpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/ta2n.jpg"><div id="ta2njpg-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('ta2njpg-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="ta2njpg-modal-img"> </div> <script>var modal=document.getElementById("ta2njpg-modal"),img=document.getElementById("ta2njpg"),modalImg=document.getElementById("ta2njpg-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="li2022ta2n" class="col-sm-8"> <div class="title">Ta2n: Two-stage action alignment network for few-shot action recognition</div> <div class="author"> Shuyuan Li, Huabin Liu, Rui Qian, Yuxi Li, John See, Mengjuan Fei, Xiaoyuan Yu, and Weiyao Lin</div> <div class="periodical"> <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20029" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://github.com/R00Kie-Liu/TA2N" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right" data-doi="10.1609/aaai.v36i2.20029"></span> <span class="__dimensions_badge_embed__" data-doi="10.1609/aaai.v36i2.20029" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1609/aaai.v36i2.20029" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" target="_blank" rel="noopener noreferrer"></a> </div> <div class="abstract hidden"> <p>Few-shot action recognition aims to recognize novel action classes (query) using just a few samples (support). The majority of current approaches follow the metric learning paradigm, which learns to compare the similarity between videos. Recently, it has been observed that directly measuring this similarity is not ideal since different action instances may show distinctive temporal distribution, resulting in severe misalignment issues across query and support videos. In this paper, we arrest this problem from two distinct aspects–action duration misalignment and action evolution misalignment. We address them sequentially through a Two-stage Action Alignment Network (TA2N). The first stage locates the action by learning a temporal affine transform, which warps each video feature to its action duration while dismissing the action-irrelevant feature (eg background). Next, the second stage coordinates query feature to match the spatial-temporal action evolution of support by performing temporally rearrange and spatially offset prediction. Extensive experiments on benchmark datasets show the potential of the proposed method in achieving state-of-the-art performance for few-shot action recognition.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <img id="needle-meviewjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/needle-meview.jpg"><div id="needle-meviewjpg-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('needle-meviewjpg-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="needle-meviewjpg-modal-img"> </div> <script>var modal=document.getElementById("needle-meviewjpg-modal"),img=document.getElementById("needle-meviewjpg"),modalImg=document.getElementById("needle-meviewjpg-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="gan2022needle" class="col-sm-8"> <div class="title">Needle in a Haystack: Spotting and recognising micro-expressions “in the wild”</div> <div class="author"> Y.S. Gan, John See, Huai-Qian Khor, Kun-Hong Liu, and Sze-Teng Liong</div> <div class="periodical"> <em>Neurocomputing</em>, 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231222008323" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right" data-doi="10.1016/j.neucom.2022.06.101"></span> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.neucom.2022.06.101" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1016/j.neucom.2022.06.101" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" target="_blank" rel="noopener noreferrer"></a> </div> <div class="abstract hidden"> <p>Computational research on facial micro-expressions has long focused on videos captured under constrained laboratory conditions due to the challenging elicitation process and limited samples that are publicly available. Moreover, processing micro-expressions is extremely challenging under unconstrained scenarios. This paper introduces, for the first time, a completely automatic micro-expression “spot-and-recognize” framework that is performed on in-the-wild videos, such as in poker games and political interviews. The proposed method first spots the apex frame from a video by handling head movements and unconscious actions which are typically larger in motion intensity, with alignment employed to enforce a canonical face pose. Optical flow guided features play a central role in our method: they can robustly identify the location of the apex frame, and are used to learn a shallow neural network model for emotion classification. Experimental results demonstrate the feasibility of the proposed methodology, establishing good baselines for both spotting and recognition tasks – ASR of 0.33 and F1-score of 0.6758 respectively on the MEVIEW micro-expression database. In addition, we present comprehensive qualitative and quantitative analyses to further show the effectiveness of the proposed framework, with new suggestion for an appropriate evaluation protocol. In a nutshell, this paper provides a new benchmark for apex spotting and emotion recognition in an in-the-wild setting.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <img id="patch-tree-decoderjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/patch-tree-decoder.jpg"><div id="patch-tree-decoderjpg-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('patch-tree-decoderjpg-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="patch-tree-decoderjpg-modal-img"> </div> <script>var modal=document.getElementById("patch-tree-decoderjpg-modal"),img=document.getElementById("patch-tree-decoderjpg"),modalImg=document.getElementById("patch-tree-decoderjpg-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="fan2022speed" class="col-sm-8"> <div class="title">Speed up object detection on gigapixel-level images with patch arrangement</div> <div class="author"> Jiahao Fan, Huabin Liu, Wenjie Yang, John See, Aixin Zhang, and Weiyao Lin</div> <div class="periodical"> <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Fan_Speed_Up_Object_Detection_on_Gigapixel-Level_Images_With_Patch_Arrangement_CVPR_2022_paper.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right" data-doi="10.1109/CVPR52688.2022.00461"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/CVPR52688.2022.00461" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1109/CVPR52688.2022.00461" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" target="_blank" rel="noopener noreferrer"></a> </div> <div class="abstract hidden"> <p>With the appearance of super high-resolution (e.g., gigapixel-level) images, performing efficient object detection on such images becomes an important issue. Most existing works for efficient object detection on high-resolution images focus on generating local patches where objects may exist, and then every patch is detected independently. However, when the image resolution reaches gigapixel-level, they will suffer from a huge time cost for detecting numerous patches. Different from them, we devise a novel patch arrangement framework for fast object detection on gigapixel-level images. Under this framework, a Patch Arrangement Network (PAN) is proposed to accelerate the detection by determining which patches could be packed together into a compact canvas. Specifically, PAN consists of (1) a Patch Filter Module (PFM) (2) a Patch Packing Module (PPM). PFM filters patch candidates by learning to select patches between two granularities. Subsequently, from the remaining patches, PPM determines how to pack these patches together into a smaller number of canvases. Meanwhile, it generates an ideal layout of patches on canvas. These canvases are fed to the detector to get final results. Experiments show that our method could improve the inference speed on gigapixel-level images by 5 times while maintaining great performance.</p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 muda lab. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script async src="https://cdn.plu.mx/widget-popup.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>