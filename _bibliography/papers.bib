---
---
% samples

@string{aps = {American Physical Society,}}

@book{einstein1956investigations,
  bibtex_show={true},
  title={Investigations on the Theory of the Brownian Movement},
  author={Einstein, Albert},
  year={1956},
  publisher={Courier Corporation,},
  preview={brownian-motion.gif}
}

@article{einstein1950meaning,
  abbr={AJP},
  bibtex_show={true},
  title={The meaning of relativity},
  author={Einstein, Albert and Taub, AH},
  journal={American Journal of Physics,},
  volume={18},
  number={6},
  pages={403--404},
  year={1950},
  publisher={American Association of Physics Teachers,}
}

@article{PhysRev.47.777,
  abbr={PhysRev},
  title={Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?},
  author={Einstein, A. and Podolsky, B. and Rosen, N.},
  abstract={In a complete theory there is an element corresponding to each element of reality. A sufficient condition for the reality of a physical quantity is the possibility of predicting it with certainty, without disturbing the system. In quantum mechanics in the case of two physical quantities described by non-commuting operators, the knowledge of one precludes the knowledge of the other. Then either (1) the description of reality given by the wave function in quantum mechanics is not complete or (2) these two quantities cannot have simultaneous reality. Consideration of the problem of making predictions concerning a system on the basis of measurements made on another system that had previously interacted with it leads to the result that if (1) is false then (2) is also false. One is thus led to conclude that the description of reality as given by a wave function is not complete.},
  journal={Phys. Rev.,},
  volume={47},
  issue={10},
  pages={777--780},
  numpages={0},
  year={1935},
  month={May},
  publisher=aps,
  doi={10.1103/PhysRev.47.777},
  url={http://link.aps.org/doi/10.1103/PhysRev.47.777},
  html={https://journals.aps.org/pr/abstract/10.1103/PhysRev.47.777},
  pdf={example_pdf.pdf},
  altmetric={248277}
}

@article{einstein1905molekularkinetischen,
  title={{\"U}ber die von der molekularkinetischen Theorie der W{\"a}rme geforderte Bewegung von in ruhenden Fl{\"u}ssigkeiten suspendierten Teilchen},
  author={Einstein, A.},
  journal={Annalen der physik,},
  volume={322},
  number={8},
  pages={549--560},
  year={1905},
  publisher={Wiley Online Library}
}

@article{einstein1905movement,
  abbr={Ann. Phys.},
  title={Un the movement of small particles suspended in statiunary liquids required by the molecular-kinetic theory 0f heat},
  author={Einstein, A.},
  journal={Ann. Phys.,},
  volume={17},
  pages={549--560},
  year={1905}
}

@article{einstein1905electrodynamics,
  title={On the electrodynamics of moving bodies},
  author={Einstein, A.},
  year={1905}
}

@book{przibram1967letters,
  bibtex_show={true},
  title={Letters on wave mechanics},
  author={Einstein, Albert and Schrödinger, Erwin and Planck, Max and Lorentz, Hendrik Antoon and Przibram, Karl},
  year={1967},
  publisher={Vision},
  preview={wave-mechanics.gif}
}

% Muda Lab papers 

@article{liong2024sfamnet,
  author={Liong, Gen-Bing and Liong, Sze-Teng and Chan, Chee Seng and See, John},
  journal={Neurocomputing}, 
  title="{SFAMNet: A Scene Flow Attention-based Micro-expression Network}", 
  year={2024},
  volume={566},
  number={},
  pages={126998},
  publisher={Elsevier},
  doi={10.1016/j.neucom.2023.126998},
  selected={true},
  code = "https://github.com/genbing99/SFAMNet",
  preview = "sfamnet.png",
  abstract = {Tremendous progress has been made in facial Micro-Expression (ME) spotting and recognition; however, most works have focused on either spotting or recognition tasks on the 2D videos. Until recently, the estimation of the 3D motion field (a.k.a scene flow) for the ME has only become possible after the release of the multi-modal ME dataset. In this paper, we propose the first Scene Flow Attention-based Micro-expression Network, namely SFAMNet. It takes the scene flow computed using the RGB-D flow algorithm as the input and predicts the spotting confidence score and emotion labels. Specifically, SFAMNet is an attention-based end-to-end multi-stream multi-task network devised to spot and recognize the ME. Besides that, we present a data augmentation strategy to alleviate the small sample size problem during network learning. Extensive experiments are performed on three tasks: (i) ME spotting; (ii) ME recognition; and (iii) ME analysis on the multi-modal CAS(ME)^3 dataset. Empirical results indicate that depth is vital in capturing the ME information and the effectiveness of the proposed approach. Our source code is publicly available at <a href="https://github.com/genbing99/SFAMNet">https://github.com/genbing99/SFAMNet</a>.},
  url = "https://ieeexplore.ieee.org/document/10015091",
  abbr = "NeuComp",
  pdf = "https://www.techrxiv.org/articles/preprint/NeSVoR_Implicit_Neural_Representation_for_Slice-to-Volume_Reconstruction_in_MRI/21398868"
}

@article{qian2024controllable,
  title={Controllable augmentations for video representation learning},
  author={Qian, Rui and Lin, Weiyao and See, John and Li, Dian},
  journal={Visual Intelligence},
  volume={2},
  number={1},
  pages={1},
  year={2024},
  publisher={Springer},
  abbr = "VisIntell",
  doi = {10.1007/s44267-023-00034-7},
  url = "https://link.springer.com/article/10.1007/s44267-023-00034-7",
  abstract = {This paper focuses on self-supervised video representation learning. Most existing approaches follow the contrastive learning pipeline to construct positive and negative pairs by sampling different clips. However, this formulation tends to bias the static background and has difficulty establishing global temporal structures. The major reason is that the positive pairs, i.e., different clips sampled from the same video, have limited temporal receptive fields, and usually share similar backgrounds but differ in motions. To address these problems, we propose a framework to jointly utilize local clips and global videos to learn from detailed region-level correspondence as well as general long-term temporal relations. Based on a set of designed controllable augmentations, we implement accurate appearance and motion pattern alignment through soft spatio-temporal region contrast. Our formulation avoids the low-level redundancy shortcut with an adversarial mutual information minimization objective to improve the generalization ability. Moreover, we introduce local-global temporal order dependency to further bridge the gap between clip-level and video-level representations for robust temporal modeling. Extensive experiments demonstrate that our framework is superior on three video benchmarks in action recognition and video retrieval, and captures more accurate temporal dynamics.},
  preview = "controllable-augs.jpg",
}

@article{yang2023doing,
  author={Yang, Cong and Yang, Zhenyu and Ke, Yan and Chen, Tao and Grzegorzek, Marcin and See, John},
  journal={IEEE Transactions on Image Processing},
  title={Doing More With Moir{\'e} Pattern Detection in Digital Photos},  
  year={2023},
  volume={32},
  pages={694--708},
  publisher={IEEE},
  doi={10.1109/TIP.2022.3232232},
  selected={true},
  code = "https://github.com/cong-yang/MoireDet",
  preview = "moiredet.png",
  abstract = {Detecting moiré patterns in digital photographs is meaningful as it provides priors towards image quality evaluation and demoiréing tasks. In this paper, we present a simple yet efficient framework to extract moiré edge maps from images with moiré patterns. The framework includes a strategy for training triplet (natural image, moiré layer, and their synthetic mixture) generation, and a Moiré Pattern Detection Neural Network (MoireDet) for moiré edge map estimation. This strategy ensures consistent pixel-level alignments during training, accommodating characteristics of a diverse set of camera-captured screen images and real-world moiré patterns from natural images. The design of three encoders in MoireDet exploits both high-level contextual and low-level structural features of various moiré patterns. Through comprehensive experiments, we demonstrate the advantages of MoireDet: better identification precision of moiré images on two datasets, and a marked improvement over state-of-the-art demoiréing methods.},
  url = "https://ieeexplore.ieee.org/document/10006755",
  abbr = "TIP",
  pdf = "https://researchportal.hw.ac.uk/files/83582564/Doing_More_With_Moir_Pattern_Detection_in_Digital_Photos.pdf",
  video = "moiredet-demo.mp4"
}

@article{gohar2023slice,
  author={Gohar, Imad and Halimi, Abderrahim and See, John and Yew, Weng Kean and Yang, Cong},
  journal={Machines},
  title={Slice-Aided Defect Detection in Ultra High-Resolution Wind Turbine Blade Images},
  year={2023},
  volume={11},
  number={10},
  pages={953},
  publisher={MDPI},
  doi={10.3390/machines11100953},
  selected={true},
  code = "https://github.com/imadgohar/DTU-annotations",
  preview = "slice-wtb.png",
  abstract = {The processing of aerial images taken by drones is a challenging task due to their high resolution and the presence of small objects. The scale of the objects varies diversely depending on the position of the drone, which can result in loss of information or increased difficulty in detecting small objects. To address this issue, images are either randomly cropped or divided into small patches before training and inference. This paper proposes a defect detection framework that harnesses the advantages of slice-aided inference for small and medium-size damage on the surface of wind turbine blades. This framework enables the comparison of different slicing strategies, including a conventional patch division strategy and a more recent slice-aided hyper-inference, on several state-of-the-art deep neural network baselines for the detection of surface defects in wind turbine blade images. Our experiments provide extensive empirical results, highlighting the benefits of using the slice-aided strategy and the significant improvements made by these networks on an ultra high-resolution drone image dataset.},
  url = "https://www.mdpi.com/2075-1702/11/10/953",
  abbr = "Machines",
  pdf = "https://www.mdpi.com/2075-1702/11/10/953/pdf?version=1697117170"
}

@article{yang2022fatigueview,
  author={Yang, Cong and Yang, Zhenyu and Li, Weiyu and See, John},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  title={FatigueView: A Multi-Camera Video Dataset for Vision-Based Drowsiness Detection},
  volume={24},
  number={1},
  pages={233--246},
  year={2022},
  publisher={IEEE},
  doi={10.1109/TITS.2022.3216017},
  selected={true},
  website = "https://fatigueview.github.io",
  code = "https://github.com/FatigueView/fatigueview",
  preview = "fatigueview.png",
  abstract = {Although vision-based drowsiness detection approaches have achieved great success on empirically organized datasets, it remains far from being satisfactory for deployment in practice. One crucial issue lies in the scarcity and lack of datasets that represent the actual challenges in real-world applications, e.g. tremendous variation and aggregation of visual signs, challenges brought on by different camera positions and camera types. To promote research in this field, we introduce a new large-scale dataset, FatigueView, that is collected by both RGB and infrared (IR) cameras from five different positions. It contains real sleepy driving videos and various visual signs of drowsiness from subtle to obvious, e.g. with 17,403 different yawning sets totaling more than 124 million frames, far more than recent actively used datasets. We also provide hierarchical annotations for each video, ranging from spatial face landmarks and visual signs to temporal drowsiness locations and levels to meet different research requirements. We structurally evaluate representative methods to build viable baselines. With FatigueView, we would like to encourage the community to adapt computer vision models to address practical real-world concerns, particularly the challenges posed by this dataset.},
  url = "https://ieeexplore.ieee.org/abstract/document/9931532",
  abbr = "TITS",
  pdf = "https://pure.hw.ac.uk/ws/files/67344144/FatigueView_A_Multi_Camera_Video_Dataset_for_Vision_Based_Drowsiness_Detection.pdf"
}

@article{yang2023skeleton,
  title={Skeleton Ground Truth Extraction: Methodology, Annotation Tool and Benchmarks},
  author={Yang, Cong and Indurkhya, Bipin and See, John and Gao, Bo and Ke, Yan and Boukhers, Zeyd and Yang, Zhenyu and Grzegorzek, Marcin},
  journal={International Journal of Computer Vision},
  pages={1--23},
  year={2023},
  publisher={Springer},
  doi={10.1007/s11263-023-01926-3},
  selected={true},
  url = "https://link.springer.com/article/10.1007/s11263-023-01926-3",
  abbr = "IJCV",
  preview = "skeletonGT.png",
  pdf = "https://researchportal.hw.ac.uk/files/104065760/s11263-023-01926-3.pdf",
  abstract = {Skeleton Ground Truth (GT) is critical to the success of supervised skeleton extraction methods, especially with the popularity of deep learning techniques. Furthermore, we see skeleton GTs used not only for training skeleton detectors with Convolutional Neural Networks (CNN), but also for evaluating skeleton-related pruning and matching algorithms. However, most existing shape and image datasets suffer from the lack of skeleton GT and inconsistency of GT standards. As a result, it is difficult to evaluate and reproduce CNN-based skeleton detectors and algorithms on a fair basis. In this paper, we present a heuristic strategy for object skeleton GT extraction in binary shapes and natural images. Our strategy is built on an extended theory of diagnosticity hypothesis, which enables encoding human-in-the-loop GT extraction based on clues from the target’s context, simplicity, and completeness. Using this strategy, we developed a tool, SkeView, to generate skeleton GT of 17 existing shape and image datasets. The GTs are then structurally evaluated with representative methods to build viable baselines for fair comparisons. Experiments demonstrate that GTs generated by our strategy yield promising quality with respect to standard consistency, and also provide a balance between simplicity and completeness.},
  code = "https://github.com/cong-yang/skeview",
  arXiv = "2310.06437"
}

@article{lim2023ernet,
  title={ERNet: An Efficient and Reliable Human-Object Interaction Detection Network},
  author={Lim, JunYi and Baskaran, Vishnu Monn and Lim, Joanne Mun-Yee and Wong, KokSheik and See, John and Tistarelli, Massimo},
  journal={IEEE Transactions on Image Processing},
  volume={32},
  pages={964--979},
  year={2023},
  publisher={IEEE},
  abbr = "TIP",
  doi={10.1109/TIP.2022.3231528},
  selected={true},
  preview = "ernet.png",
  abstract = {Human-Object Interaction (HOI) detection recognizes how persons interact with objects, which is advantageous in autonomous systems such as self-driving vehicles and collaborative robots. However, current HOI detectors are often plagued by model inefficiency and unreliability when making a prediction, which consequently limits its potential for real-world scenarios. In this paper, we address these challenges by proposing ERNet, an end-to-end trainable convolutional-transformer network for HOI detection. The proposed model employs an efficient multi-scale deformable attention to effectively capture vital HOI features. We also put forward a novel detection attention module to adaptively generate semantically rich instance and interaction tokens. These tokens undergo pre-emptive detections to produce initial region and vector proposals that also serve as queries which enhances the feature refinement process in the transformer decoders. Several impactful enhancements are also applied to improve the HOI representation learning. Additionally, we utilize a predictive uncertainty estimation framework in the instance and interaction classification heads to quantify the uncertainty behind each prediction. By doing so, we can accurately and reliably predict HOIs even under challenging scenarios. Experiment results on the HICO-Det, V-COCO, and HOI-A datasets demonstrate that the proposed model achieves state-of-the-art performance in detection accuracy and training efficiency. Codes are publicly available at <a href="https://github.com/Monash-CyPhi-AI-Research-Lab/ernet">https://github.com/Monash-CyPhi-AI-Research-Lab/ernet</a>.},
  code = "https://github.com/Monash-CyPhi-AI-Research-Lab/ernet",
}

@inproceedings{davison2023megc2023,
  title={MEGC2023: ACM Multimedia 2023 ME Grand Challenge},
  author={Davison, Adrian K and Li, Jingting and Yap, Moi Hoon and See, John and Cheng, Wen-Huang and Li, Xiaobai and Hong, Xiaopeng and Wang, Su-Jing},
  booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
  pages={9625--9629},
  year={2023},
  doi={10.1145/3581783.3612833},
  abbr="MM",
  preview = "megc2023.png",
  selected = {true},
  publisher = "ACM",
  abstract = {Facial micro-expressions (MEs) are involuntary movements of the face that occur spontaneously when a person experiences an emotion but attempts to suppress or repress the facial expression, typically found in a high-stakes environment. Unfortunately, the small sample problem severely limits the automation of ME analysis. Furthermore, due to the weak and transient nature of MEs, it is difficult for models to distinguish it from other types of facial actions. Therefore, ME in long videos is a challenging task, and the current performance cannot meet the practical application requirements. Addressing these issues, this challenge focuses on ME and the macro-expression (MaE) spotting task. This year, in order to evaluate algorithms' performance more fairly, based on CAS(ME)2, SAMM Long Videos, SMIC-E-long, CAS(ME)3 and 4DME, we build an unseen cross-cultural long-video test set. All participating algorithms are required to run on this test set and submit their results on a leaderboard with a baseline result.},
  url = "https://dl.acm.org/doi/abs/10.1145/3581783.3612833",
  website = "https://megc2023.github.io/",
  isbn="979-8-4007-0108-5/23/10"
}

@article{yang2023towards,
  title={Towards accurate image stitching for drone-based wind turbine blade inspection},
  author={Yang, Cong and Liu, Xun and Zhou, Hua and Ke, Yan and See, John},
  journal={Renewable Energy},
  volume={203},
  pages={267--279},
  year={2023},
  publisher={Elsevier},
  selected = {true},
  doi = {10.1016/j.renene.2022.12.063},
  preview = "blade-stitching.jpg",
  abstract = {Accurate image stitching is crucial to wind turbine blade visualization and defect analysis. It is inevitable that drone-captured images for blade inspection are high resolution and heavily overlapped. This also necessitates the stitching-based deduplication process on detected defects. However, the stitching task suffers from texture-poor blade surfaces, unstable drone pose (especially off-shore), and the lack of public blade datasets that cater to real-world challenges. In this paper, we present a simple yet efficient algorithm for robust and accurate blade image stitching. To promote further research, we also introduce a new dataset, Blade30, which contains 1,302 real drone-captured images covering 30 full blades captured under various conditions (both on- and off-shore), accompanied by a rich set of annotations such as defects and contaminations, etc. The proposed stitching algorithm generates the initial blade panorama based on blade edges and drone-blade distances at the coarse-grained level, followed by fine-grained adjustments optimized by regression-based texture and shape losses. Our method also fully utilizes the properties of blade images and prior information of the drone. Experiments report promising accuracy in blade stitching and defect deduplication tasks in the vision-based wind turbine blade inspection scenario, surpassing the performance of existing methods.},
  dataset = "https://github.com/cong-yang/Blade30",
  url = "https://www.sciencedirect.com/science/article/pii/S0960148122018481",
  abbr = "RenwEnergy"
}

@inproceedings{chong2023modality,
  title={What Modality Matters? Exploiting Highly Relevant Features for Video Advertisement Insertion},
  author={Chong, Onn Keat and Goh, Hui-Ngo and See, John},
  booktitle={IEEE International Conference on Image Processing (ICIP)},
  pages={3344--3348},
  year={2023},
  organization={IEEE},
  abbr = "ICIP",
  preview = "modality-ad-insertion.jpg",
  abstract = {Video advertising is a thriving industry that has recently turned its attention to the use of intelligent algorithms for automating tasks. In advertisement insertion, the integration of contextual relevance is essential in influencing the viewer’s experience. Despite the wide spectrum of audio-visual semantic modalities available, there is a lack of research that analyzes their individual and complementary strengths in a systematic manner. In this paper, we propose an ad-insertion framework that maximizes the contextual relevance between advertisement and content video by employing high-level multi-modal semantic features. Prediction vectors are derived via clip-level and image-level extractors, which are then matched accordingly to yield relevance scores. We also established a new user study methodology that produces gold standard annotations based on multiple expert selections. By comprehensive human-centered approaches and analysis, we demonstrate that automatic ad-insertion can be improved by exploiting effective combinations of semantic modalities.},
  url = "https://ieeexplore.ieee.org/abstract/document/10222693",
  doi = {10.1109/ICIP49359.2023.10222693},
}

@inproceedings{nagappan2023context,
  title={Context-Aware Multi-Stream Networks for Dimensional Emotion Prediction in Images},
  author={Nagappan, Sidharrth and Tan, Jia Qi and Wong, Lai-Kuan and See, John},
  booktitle={IEEE International Conference on Image Processing (ICIP)},
  pages={2480--2484},
  year={2023},
  organization={IEEE},
  abbr = "ICIP",
  preview = "context-aware.jpg",
  abstract = {Teaching machines to comprehend the nuances of emotion from photographs is a particularly challenging task. Emotion perception— naturally a subjective problem, is often simplified for computational purposes into categorical states or valence-arousal dimensional space, the latter being a lesser-explored problem in the literature. This paper proposes a multi-stream context-aware neural network model for dimensional emotion prediction in images. Models were trained using a set of object and scene data along with deep features for valence, arousal, and dominance estimation. Experimental evaluation on a large-scale image emotion dataset demonstrates the viability of our proposed approach. Our analysis postulates that the understanding of the depicted object in an image is vital for successful predictions whilst relying on scene information can lead to somewhat confounding effects.},
  url = "https://ieeexplore.ieee.org/abstract/document/10221960",
  doi = {10.1109/ICIP49359.2023.10221960},
}

