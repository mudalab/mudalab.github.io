---
---

@string{aps = {American Physical Society,}}

@book{einstein1956investigations,
  bibtex_show={true},
  title={Investigations on the Theory of the Brownian Movement},
  author={Einstein, Albert},
  year={1956},
  publisher={Courier Corporation,},
  preview={brownian-motion.gif}
}

@article{einstein1950meaning,
  abbr={AJP},
  bibtex_show={true},
  title={The meaning of relativity},
  author={Einstein, Albert and Taub, AH},
  journal={American Journal of Physics,},
  volume={18},
  number={6},
  pages={403--404},
  year={1950},
  publisher={American Association of Physics Teachers,}
}

@article{PhysRev.47.777,
  abbr={PhysRev},
  title={Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?},
  author={Einstein, A. and Podolsky, B. and Rosen, N.},
  abstract={In a complete theory there is an element corresponding to each element of reality. A sufficient condition for the reality of a physical quantity is the possibility of predicting it with certainty, without disturbing the system. In quantum mechanics in the case of two physical quantities described by non-commuting operators, the knowledge of one precludes the knowledge of the other. Then either (1) the description of reality given by the wave function in quantum mechanics is not complete or (2) these two quantities cannot have simultaneous reality. Consideration of the problem of making predictions concerning a system on the basis of measurements made on another system that had previously interacted with it leads to the result that if (1) is false then (2) is also false. One is thus led to conclude that the description of reality as given by a wave function is not complete.},
  journal={Phys. Rev.,},
  volume={47},
  issue={10},
  pages={777--780},
  numpages={0},
  year={1935},
  month={May},
  publisher=aps,
  doi={10.1103/PhysRev.47.777},
  url={http://link.aps.org/doi/10.1103/PhysRev.47.777},
  html={https://journals.aps.org/pr/abstract/10.1103/PhysRev.47.777},
  pdf={example_pdf.pdf},
  altmetric={248277},
  selected={true}
}

@article{einstein1905molekularkinetischen,
  title={{\"U}ber die von der molekularkinetischen Theorie der W{\"a}rme geforderte Bewegung von in ruhenden Fl{\"u}ssigkeiten suspendierten Teilchen},
  author={Einstein, A.},
  journal={Annalen der physik,},
  volume={322},
  number={8},
  pages={549--560},
  year={1905},
  publisher={Wiley Online Library}
}

@article{einstein1905movement,
  abbr={Ann. Phys.},
  title={Un the movement of small particles suspended in statiunary liquids required by the molecular-kinetic theory 0f heat},
  author={Einstein, A.},
  journal={Ann. Phys.,},
  volume={17},
  pages={549--560},
  year={1905}
}

@article{einstein1905electrodynamics,
  title={On the electrodynamics of moving bodies},
  author={Einstein, A.},
  year={1905}
}

@book{przibram1967letters,
  bibtex_show={true},
  title={Letters on wave mechanics},
  author={Einstein, Albert and Schrödinger, Erwin and Planck, Max and Lorentz, Hendrik Antoon and Przibram, Karl},
  year={1967},
  publisher={Vision},
  preview={wave-mechanics.gif}
}

@article{liong2024sfamnet,
  author={Liong, Gen-Bing and Liong, Sze-Teng and Chan, Chee Seng and See, John},
  journal={Neurocomputing}, 
  title="{SFAMNet: A Scene Flow Attention-based Micro-expression Network}", 
  year={2024},
  volume={566},
  number={},
  pages={126998},
  publisher={Elsevier},
  doi={10.1016/j.neucom.2023.126998},
  selected={true},
  code = "https://github.com/genbing99/SFAMNet",
  preview = "sfamnet.png",
  abstract = {Tremendous progress has been made in facial Micro-Expression (ME) spotting and recognition; however, most works have focused on either spotting or recognition tasks on the 2D videos. Until recently, the estimation of the 3D motion field (a.k.a scene flow) for the ME has only become possible after the release of the multi-modal ME dataset. In this paper, we propose the first Scene Flow Attention-based Micro-expression Network, namely SFAMNet. It takes the scene flow computed using the RGB-D flow algorithm as the input and predicts the spotting confidence score and emotion labels. Specifically, SFAMNet is an attention-based end-to-end multi-stream multi-task network devised to spot and recognize the ME. Besides that, we present a data augmentation strategy to alleviate the small sample size problem during network learning. Extensive experiments are performed on three tasks: (i) ME spotting; (ii) ME recognition; and (iii) ME analysis on the multi-modal CAS(ME)^3 dataset. Empirical results indicate that depth is vital in capturing the ME information and the effectiveness of the proposed approach. Our source code is publicly available at <a href="https://github.com/genbing99/SFAMNet">https://github.com/genbing99/SFAMNet</a>.},
  url = "https://ieeexplore.ieee.org/document/10015091",
  abbr = "NeuComp",
  pdf = "https://www.techrxiv.org/articles/preprint/NeSVoR_Implicit_Neural_Representation_for_Slice-to-Volume_Reconstruction_in_MRI/21398868"
}

@article{yang2023doing,
  author={Yang, Cong and Yang, Zhenyu and Ke, Yan and Chen, Tao and Grzegorzek, Marcin and See, John},
  journal={IEEE Transactions on Image Processing},
  title={Doing More With Moir{\'e} Pattern Detection in Digital Photos},  
  year={2023},
  volume={32},
  pages={694--708},
  publisher={IEEE},
  doi={10.1109/TIP.2022.3232232},
  selected={true},
  code = "https://github.com/cong-yang/MoireDet",
  preview = "moiredet.png",
  abstract = {Detecting moiré patterns in digital photographs is meaningful as it provides priors towards image quality evaluation and demoiréing tasks. In this paper, we present a simple yet efficient framework to extract moiré edge maps from images with moiré patterns. The framework includes a strategy for training triplet (natural image, moiré layer, and their synthetic mixture) generation, and a Moiré Pattern Detection Neural Network (MoireDet) for moiré edge map estimation. This strategy ensures consistent pixel-level alignments during training, accommodating characteristics of a diverse set of camera-captured screen images and real-world moiré patterns from natural images. The design of three encoders in MoireDet exploits both high-level contextual and low-level structural features of various moiré patterns. Through comprehensive experiments, we demonstrate the advantages of MoireDet: better identification precision of moiré images on two datasets, and a marked improvement over state-of-the-art demoiréing methods.},
  url = "https://ieeexplore.ieee.org/document/10006755",
  abbr = "TIP",
  pdf = "https://researchportal.hw.ac.uk/files/83582564/Doing_More_With_Moir_Pattern_Detection_in_Digital_Photos.pdf",
  video = "moiredet-demo.mp4"
}

@article{gohar2023slice,
  author={Gohar, Imad and Halimi, Abderrahim and See, John and Yew, Weng Kean and Yang, Cong},
  journal={Machines},
  title={Slice-Aided Defect Detection in Ultra High-Resolution Wind Turbine Blade Images},
  year={2023},
  volume={11},
  number={10},
  pages={953},
  publisher={MDPI},
  doi={10.3390/machines11100953},
  selected={true},
  code = "https://github.com/imadgohar/DTU-annotations",
  preview = "slice-wtb.png",
  abstract = {The processing of aerial images taken by drones is a challenging task due to their high resolution and the presence of small objects. The scale of the objects varies diversely depending on the position of the drone, which can result in loss of information or increased difficulty in detecting small objects. To address this issue, images are either randomly cropped or divided into small patches before training and inference. This paper proposes a defect detection framework that harnesses the advantages of slice-aided inference for small and medium-size damage on the surface of wind turbine blades. This framework enables the comparison of different slicing strategies, including a conventional patch division strategy and a more recent slice-aided hyper-inference, on several state-of-the-art deep neural network baselines for the detection of surface defects in wind turbine blade images. Our experiments provide extensive empirical results, highlighting the benefits of using the slice-aided strategy and the significant improvements made by these networks on an ultra high-resolution drone image dataset.},
  url = "https://www.mdpi.com/2075-1702/11/10/953",
  abbr = "Machines",
  pdf = "https://www.mdpi.com/2075-1702/11/10/953/pdf?version=1697117170"
}

@article{yang2022fatigueview,
  author={Yang, Cong and Yang, Zhenyu and Li, Weiyu and See, John},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  title={FatigueView: A Multi-Camera Video Dataset for Vision-Based Drowsiness Detection},
  volume={24},
  number={1},
  pages={233--246},
  year={2022},
  publisher={IEEE},
  doi={10.1109/TITS.2022.3216017},
  selected={true},
  website = "https://fatigueview.github.io",
  code = "https://github.com/FatigueView/fatigueview",
  preview = "fatigueview.png",
  abstract = {Although vision-based drowsiness detection approaches have achieved great success on empirically organized datasets, it remains far from being satisfactory for deployment in practice. One crucial issue lies in the scarcity and lack of datasets that represent the actual challenges in real-world applications, e.g. tremendous variation and aggregation of visual signs, challenges brought on by different camera positions and camera types. To promote research in this field, we introduce a new large-scale dataset, FatigueView, that is collected by both RGB and infrared (IR) cameras from five different positions. It contains real sleepy driving videos and various visual signs of drowsiness from subtle to obvious, e.g. with 17,403 different yawning sets totaling more than 124 million frames, far more than recent actively used datasets. We also provide hierarchical annotations for each video, ranging from spatial face landmarks and visual signs to temporal drowsiness locations and levels to meet different research requirements. We structurally evaluate representative methods to build viable baselines. With FatigueView, we would like to encourage the community to adapt computer vision models to address practical real-world concerns, particularly the challenges posed by this dataset.},
  url = "https://ieeexplore.ieee.org/abstract/document/9931532",
  abbr = "TITS",
  pdf = "https://pure.hw.ac.uk/ws/files/67344144/FatigueView_A_Multi_Camera_Video_Dataset_for_Vision_Based_Drowsiness_Detection.pdf"
}

@article{yang2023skeleton,
  title={Skeleton Ground Truth Extraction: Methodology, Annotation Tool and Benchmarks},
  author={Yang, Cong and Indurkhya, Bipin and See, John and Gao, Bo and Ke, Yan and Boukhers, Zeyd and Yang, Zhenyu and Grzegorzek, Marcin},
  journal={International Journal of Computer Vision},
  pages={1--23},
  year={2023},
  publisher={Springer},
  doi={10.1007/s11263-023-01926-3},
  selected={true},
  url = "https://link.springer.com/article/10.1007/s11263-023-01926-3",
  abbr = "IJCV",
  preview = "skeletonGT.png",
  pdf = "https://researchportal.hw.ac.uk/files/104065760/s11263-023-01926-3.pdf",
  abstract = {Skeleton Ground Truth (GT) is critical to the success of supervised skeleton extraction methods, especially with the popularity of deep learning techniques. Furthermore, we see skeleton GTs used not only for training skeleton detectors with Convolutional Neural Networks (CNN), but also for evaluating skeleton-related pruning and matching algorithms. However, most existing shape and image datasets suffer from the lack of skeleton GT and inconsistency of GT standards. As a result, it is difficult to evaluate and reproduce CNN-based skeleton detectors and algorithms on a fair basis. In this paper, we present a heuristic strategy for object skeleton GT extraction in binary shapes and natural images. Our strategy is built on an extended theory of diagnosticity hypothesis, which enables encoding human-in-the-loop GT extraction based on clues from the target’s context, simplicity, and completeness. Using this strategy, we developed a tool, SkeView, to generate skeleton GT of 17 existing shape and image datasets. The GTs are then structurally evaluated with representative methods to build viable baselines for fair comparisons. Experiments demonstrate that GTs generated by our strategy yield promising quality with respect to standard consistency, and also provide a balance between simplicity and completeness.},
  code = "https://github.com/cong-yang/skeview",
  arXiv = "2310.06437"
}

@article{lim2023ernet,
  title={ERNet: An Efficient and Reliable Human-Object Interaction Detection Network},
  author={Lim, JunYi and Baskaran, Vishnu Monn and Lim, Joanne Mun-Yee and Wong, KokSheik and See, John and Tistarelli, Massimo},
  journal={IEEE Transactions on Image Processing},
  volume={32},
  pages={964--979},
  year={2023},
  publisher={IEEE},
  abbr = "TIP",
  doi={10.1109/TIP.2022.3231528},
  selected={true},
  preview = "ernet.png",
  abstract = {Human-Object Interaction (HOI) detection recognizes how persons interact with objects, which is advantageous in autonomous systems such as self-driving vehicles and collaborative robots. However, current HOI detectors are often plagued by model inefficiency and unreliability when making a prediction, which consequently limits its potential for real-world scenarios. In this paper, we address these challenges by proposing ERNet, an end-to-end trainable convolutional-transformer network for HOI detection. The proposed model employs an efficient multi-scale deformable attention to effectively capture vital HOI features. We also put forward a novel detection attention module to adaptively generate semantically rich instance and interaction tokens. These tokens undergo pre-emptive detections to produce initial region and vector proposals that also serve as queries which enhances the feature refinement process in the transformer decoders. Several impactful enhancements are also applied to improve the HOI representation learning. Additionally, we utilize a predictive uncertainty estimation framework in the instance and interaction classification heads to quantify the uncertainty behind each prediction. By doing so, we can accurately and reliably predict HOIs even under challenging scenarios. Experiment results on the HICO-Det, V-COCO, and HOI-A datasets demonstrate that the proposed model achieves state-of-the-art performance in detection accuracy and training efficiency. Codes are publicly available at <a href="https://github.com/Monash-CyPhi-AI-Research-Lab/ernet">https://github.com/Monash-CyPhi-AI-Research-Lab/ernet</a>.},
  code = "https://github.com/Monash-CyPhi-AI-Research-Lab/ernet",
}

@inproceedings{davison2023megc2023,
  title={MEGC2023: ACM Multimedia 2023 ME Grand Challenge},
  author={Davison, Adrian K and Li, Jingting and Yap, Moi Hoon and See, John and Cheng, Wen-Huang and Li, Xiaobai and Hong, Xiaopeng and Wang, Su-Jing},
  booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
  pages={9625--9629},
  year={2023},
  doi={10.1145/3581783.3612833},
  abbr="MM",
  preview = "megc2023.png",
  selected = {true},
  publisher = "ACM",
  abstract = {Facial micro-expressions (MEs) are involuntary movements of the face that occur spontaneously when a person experiences an emotion but attempts to suppress or repress the facial expression, typically found in a high-stakes environment. Unfortunately, the small sample problem severely limits the automation of ME analysis. Furthermore, due to the weak and transient nature of MEs, it is difficult for models to distinguish it from other types of facial actions. Therefore, ME in long videos is a challenging task, and the current performance cannot meet the practical application requirements. Addressing these issues, this challenge focuses on ME and the macro-expression (MaE) spotting task. This year, in order to evaluate algorithms' performance more fairly, based on CAS(ME)2, SAMM Long Videos, SMIC-E-long, CAS(ME)3 and 4DME, we build an unseen cross-cultural long-video test set. All participating algorithms are required to run on this test set and submit their results on a leaderboard with a baseline result.},
  url = "https://dl.acm.org/doi/abs/10.1145/3581783.3612833",
  website = "https://megc2023.github.io/",
  isbn="979-8-4007-0108-5/23/10"
}