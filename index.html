<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>muda lab</title> <meta name="author" content="muda lab"/> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://mudalab.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">muda</span> lab </h1> <p class="desc"><a href="#">Multimedia Data Analysis Lab</a></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/hwum-lake-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/hwum-lake-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/hwum-lake-1400.webp"></source> <img src="/assets/img/hwum-lake.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="hwum-lake.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="address"> <p>School of Mathematical and Computer Sciences</p> <p>Heriot-Watt University Malaysia</p> <p>Putrajaya 62200, Malaysia</p> </div> </div> <div class="clearfix"> <p>Welcome to the Multimedia Data Analysis (MuDA) Lab! MuDA Lab is aa research group started in 2022, headed by <a href="https://johnsee.net" target="_blank" rel="noopener noreferrer">Dr. John See</a>. We are based in <a href="https://www.hw.ac.uk/malaysia/" target="_blank" rel="noopener noreferrer">Heriot-Watt University</a> (Malaysia campus), and we work closely with collaborators from the <a href="https://viprlab.github.io" target="_blank" rel="noopener noreferrer">Visual Processing Lab (MMU)</a> and <a href="https://weiyaolin.github.io" target="_blank" rel="noopener noreferrer">Shanghai Jiao Tong University (SJTU)</a>.</p> <p><b>Research statement:</b> Our principal research interests lie in the advancement of multimedia signal processing particularly in discovering new methodologies and techniques in emotional analysis and multimodal systems, for solving real-world problems in various applications such as micro-expression detection, human-robot interaction, multimodal content generation, and more.</p> </div> <div class="news"> <h2>news</h2> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Aug 22, 2024</th> <td> We have four papers accepted to various tracks in <a href="https://2024.acmmm.org/" target="_blank" rel="noopener noreferrer">ACM MM 2024</a>! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> </td> </tr> <tr> <th scope="row">May 22, 2024</th> <td> Paper on rotated WTB defect detection by Imad accepted to <a href="https://eusipcolyon.sciencesconf.org/" target="_blank" rel="noopener noreferrer">EUSIPCO 2024</a>. </td> </tr> <tr> <th scope="row">May 16, 2024</th> <td> <a class="news-title" href="/news/20240516-JiaYap-at-PGR-Conference/">Congratulations Jia Yap for an award at the HWUM PGR Conference</a> </td> </tr> <tr> <th scope="row">Jan 1, 2024</th> <td> John See to serve as Subject Editor (Senior Associate Editor) of <a href="https://www.sciencedirect.com/journal/signal-processing" target="_blank" rel="noopener noreferrer">Signal Processing</a> and Associate Editor of <a href="https://academic.oup.com/comjnl" target="_blank" rel="noopener noreferrer">The Computer Journal</a> starting from 2024. </td> </tr> </table> </div> </div> <div class="publications"> <h2>selected publications</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <img id="sfamnetpng" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/sfamnet.png"><div id="sfamnetpng-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('sfamnetpng-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="sfamnetpng-modal-img"> </div> <script>var modal=document.getElementById("sfamnetpng-modal"),img=document.getElementById("sfamnetpng"),modalImg=document.getElementById("sfamnetpng-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="liong2024sfamnet" class="col-sm-8"> <div class="title">SFAMNet: A Scene Flow Attention-based Micro-expression Network</div> <div class="author"> Gen-Bing Liong, Sze-Teng Liong, Chee Seng Chan, and John See</div> <div class="periodical"> <em>Neurocomputing</em>, 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.sciencedirect.com/science/article/pii/S0925231223011219" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://github.com/genbing99/SFAMNet" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right" data-doi="10.1016/j.neucom.2023.126998"></span> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.neucom.2023.126998" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1016/j.neucom.2023.126998" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" target="_blank" rel="noopener noreferrer"></a> </div> <div class="abstract hidden"> <p>Tremendous progress has been made in facial Micro-Expression (ME) spotting and recognition; however, most works have focused on either spotting or recognition tasks on the 2D videos. Until recently, the estimation of the 3D motion field (a.k.a scene flow) for the ME has only become possible after the release of the multi-modal ME dataset. In this paper, we propose the first Scene Flow Attention-based Micro-expression Network, namely SFAMNet. It takes the scene flow computed using the RGB-D flow algorithm as the input and predicts the spotting confidence score and emotion labels. Specifically, SFAMNet is an attention-based end-to-end multi-stream multi-task network devised to spot and recognize the ME. Besides that, we present a data augmentation strategy to alleviate the small sample size problem during network learning. Extensive experiments are performed on three tasks: (i) ME spotting; (ii) ME recognition; and (iii) ME analysis on the multi-modal CAS(ME)^3 dataset. Empirical results indicate that depth is vital in capturing the ME information and the effectiveness of the proposed approach. Our source code is publicly available at <a href="https://github.com/genbing99/SFAMNet" target="_blank" rel="noopener noreferrer">https://github.com/genbing99/SFAMNet</a>.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <img id="spot-then-recognizejpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/spot-then-recognize.jpg"><div id="spot-then-recognizejpg-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('spot-then-recognizejpg-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="spot-then-recognizejpg-modal-img"> </div> <script>var modal=document.getElementById("spot-then-recognizejpg-modal"),img=document.getElementById("spot-then-recognizejpg"),modalImg=document.getElementById("spot-then-recognizejpg-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="liong2023spot" class="col-sm-8"> <div class="title">Spot-then-recognize: A micro-expression analysis network for seamless evaluation of long videos</div> <div class="author"> Gen-Bing Liong, John See, and Chee-Seng Chan</div> <div class="periodical"> <em>Signal Processing: Image Communication</em>, 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0923596522001540" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://github.com/genbing99/MEAN_Spot-then-recognize" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right" data-doi="10.1016/j.image.2022.116875"></span> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.image.2022.116875" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1016/j.image.2022.116875" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" target="_blank" rel="noopener noreferrer"></a> </div> <div class="abstract hidden"> <p>Facial Micro-Expressions (MEs) reveal a person’s hidden emotions in high stake situations within a fraction of a second and at a low intensity. The broad range of potential real-world applications that can be applied has drawn considerable attention from researchers in recent years. However, both spotting and recognition tasks are often treated separately. In this paper, we present Micro-Expression Analysis Network (MEAN), a shallow multi-stream multi-output network architecture comprising of task-specific (spotting and recognition) networks that is designed to effectively learn a meaningful representation from both ME class labels and location-wise pseudo-labels. Notably, this is the first known work that addresses ME analysis on long videos using a deep learning approach, whereby ME spotting and recognition are performed sequentially in a two-step procedure: first spotting the ME intervals using the spotting network, and proceeding to predict their emotion classes using the recognition network. We report extensive benchmark results on the ME analysis task on both short video datasets (CASME II, SMIC-E-HS, SMIC-E-VIS, and SMIC-E-NIR), and long video datasets (CAS(ME)2 and SAMMLV); the latter in particular demonstrates the capability of the proposed approach under unconstrained settings. Besides the standard measures, we promote the usage of fairer metrics in evaluating the performance of a complete ME analysis system. We also provide visual explanations of where the network is “looking” and showcasing the effectiveness of inductive transfer applied during network training. An analysis is performed on the in-the-wild dataset (MEVIEW) to open up further research into real-world scenarios.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <img id="moiredetpng" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/moiredet.png"><div id="moiredetpng-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('moiredetpng-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="moiredetpng-modal-img"> </div> <script>var modal=document.getElementById("moiredetpng-modal"),img=document.getElementById("moiredetpng"),modalImg=document.getElementById("moiredetpng-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="yang2023doing" class="col-sm-8"> <div class="title">Doing More With Moiré Pattern Detection in Digital Photos</div> <div class="author"> Cong Yang, Zhenyu Yang, Yan Ke, Tao Chen, Marcin Grzegorzek, and John See</div> <div class="periodical"> <em>IEEE Transactions on Image Processing</em>, 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/document/10006755" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://researchportal.hw.ac.uk/files/83582564/Doing_More_With_Moir_Pattern_Detection_in_Digital_Photos.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/cong-yang/MoireDet" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right" data-doi="10.1109/TIP.2022.3232232"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/TIP.2022.3232232" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1109/TIP.2022.3232232" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" target="_blank" rel="noopener noreferrer"></a> </div> <div class="abstract hidden"> <p>Detecting moiré patterns in digital photographs is meaningful as it provides priors towards image quality evaluation and demoiréing tasks. In this paper, we present a simple yet efficient framework to extract moiré edge maps from images with moiré patterns. The framework includes a strategy for training triplet (natural image, moiré layer, and their synthetic mixture) generation, and a Moiré Pattern Detection Neural Network (MoireDet) for moiré edge map estimation. This strategy ensures consistent pixel-level alignments during training, accommodating characteristics of a diverse set of camera-captured screen images and real-world moiré patterns from natural images. The design of three encoders in MoireDet exploits both high-level contextual and low-level structural features of various moiré patterns. Through comprehensive experiments, we demonstrate the advantages of MoireDet: better identification precision of moiré images on two datasets, and a marked improvement over state-of-the-art demoiréing methods.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <img id="slice-wtbjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/slice-wtb.jpg"><div id="slice-wtbjpg-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('slice-wtbjpg-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="slice-wtbjpg-modal-img"> </div> <script>var modal=document.getElementById("slice-wtbjpg-modal"),img=document.getElementById("slice-wtbjpg"),modalImg=document.getElementById("slice-wtbjpg-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="gohar2023slice" class="col-sm-8"> <div class="title">Slice-Aided Defect Detection in Ultra High-Resolution Wind Turbine Blade Images</div> <div class="author"> Imad Gohar, Abderrahim Halimi, John See, Weng Kean Yew, and Cong Yang</div> <div class="periodical"> <em>Machines</em>, 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.mdpi.com/2075-1702/11/10/953" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://www.mdpi.com/2075-1702/11/10/953/pdf?version=1697117170" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/imadgohar/DTU-annotations" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right" data-doi="10.3390/machines11100953"></span> <span class="__dimensions_badge_embed__" data-doi="10.3390/machines11100953" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.3390/machines11100953" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" target="_blank" rel="noopener noreferrer"></a> </div> <div class="abstract hidden"> <p>The processing of aerial images taken by drones is a challenging task due to their high resolution and the presence of small objects. The scale of the objects varies diversely depending on the position of the drone, which can result in loss of information or increased difficulty in detecting small objects. To address this issue, images are either randomly cropped or divided into small patches before training and inference. This paper proposes a defect detection framework that harnesses the advantages of slice-aided inference for small and medium-size damage on the surface of wind turbine blades. This framework enables the comparison of different slicing strategies, including a conventional patch division strategy and a more recent slice-aided hyper-inference, on several state-of-the-art deep neural network baselines for the detection of surface defects in wind turbine blade images. Our experiments provide extensive empirical results, highlighting the benefits of using the slice-aided strategy and the significant improvements made by these networks on an ultra high-resolution drone image dataset.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <img id="fatigueviewpng" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/fatigueview.png"><div id="fatigueviewpng-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('fatigueviewpng-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="fatigueviewpng-modal-img"> </div> <script>var modal=document.getElementById("fatigueviewpng-modal"),img=document.getElementById("fatigueviewpng"),modalImg=document.getElementById("fatigueviewpng-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="yang2022fatigueview" class="col-sm-8"> <div class="title">FatigueView: A Multi-Camera Video Dataset for Vision-Based Drowsiness Detection</div> <div class="author"> Cong Yang, Zhenyu Yang, Weiyu Li, and John See</div> <div class="periodical"> <em>IEEE Transactions on Intelligent Transportation Systems</em>, 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/abstract/document/9931532" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://pure.hw.ac.uk/ws/files/67344144/FatigueView_A_Multi_Camera_Video_Dataset_for_Vision_Based_Drowsiness_Detection.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/FatigueView/fatigueview" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://fatigueview.github.io" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right" data-doi="10.1109/TITS.2022.3216017"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/TITS.2022.3216017" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1109/TITS.2022.3216017" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" target="_blank" rel="noopener noreferrer"></a> </div> <div class="abstract hidden"> <p>Although vision-based drowsiness detection approaches have achieved great success on empirically organized datasets, it remains far from being satisfactory for deployment in practice. One crucial issue lies in the scarcity and lack of datasets that represent the actual challenges in real-world applications, e.g. tremendous variation and aggregation of visual signs, challenges brought on by different camera positions and camera types. To promote research in this field, we introduce a new large-scale dataset, FatigueView, that is collected by both RGB and infrared (IR) cameras from five different positions. It contains real sleepy driving videos and various visual signs of drowsiness from subtle to obvious, e.g. with 17,403 different yawning sets totaling more than 124 million frames, far more than recent actively used datasets. We also provide hierarchical annotations for each video, ranging from spatial face landmarks and visual signs to temporal drowsiness locations and levels to meet different research requirements. We structurally evaluate representative methods to build viable baselines. With FatigueView, we would like to encourage the community to adapt computer vision models to address practical real-world concerns, particularly the challenges posed by this dataset.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <img id="pseudo-labelingjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/pseudo-labeling.jpg"><div id="pseudo-labelingjpg-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('pseudo-labelingjpg-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="pseudo-labelingjpg-modal-img"> </div> <script>var modal=document.getElementById("pseudo-labelingjpg-modal"),img=document.getElementById("pseudo-labelingjpg"),modalImg=document.getElementById("pseudo-labelingjpg-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="liong2022mtsn" class="col-sm-8"> <div class="title">Mtsn: A multi-temporal stream network for spotting facial macro-and micro-expression with hard and soft pseudo-labels</div> <div class="author"> Gen Bing Liong, Sze-Teng Liong, John See, and Chee-Seng Chan</div> <div class="periodical"> <em>Proceedings of the 2nd Workshop on Facial Micro-Expression: Advanced Techniques for Multi-Modal Facial Expression Analysis</em>, 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3552465.3555040" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://github.com/genbing99/MTSN-Spot-ME-MaE" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right" data-doi="10.1145/3552465.3555040"></span> <span class="__dimensions_badge_embed__" data-doi="10.1145/3552465.3555040" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1145/3552465.3555040" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" target="_blank" rel="noopener noreferrer"></a> </div> <div class="abstract hidden"> <p>This paper considers the challenge of spotting facial macro- and micro-expression from long videos. We propose the multi-temporal stream network (MTSN) model that takes two distinct inputs by considering the different temporal information in the facial movement. We also introduce a hard and soft pseudo-labeling technique to enable the network to distinguish expression frames from non-expression frames via the learning of salient features in the expression peak frame. Consequently, we demonstrate how a single output from the MTSN model can be post-processed to predict both macro- and micro-expression intervals. Our results outperform the MEGC 2022 baseline method significantly by achieving an overall F1-score of 0.2586 and also did remarkably well on the MEGC 2021 benchmark with an overall F1-score of 0.3620 and 0.2867 on CAS(ME)2 and SAMM Long Videos, respectively.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <img id="ta2njpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/ta2n.jpg"><div id="ta2njpg-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('ta2njpg-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="ta2njpg-modal-img"> </div> <script>var modal=document.getElementById("ta2njpg-modal"),img=document.getElementById("ta2njpg"),modalImg=document.getElementById("ta2njpg-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="li2022ta2n" class="col-sm-8"> <div class="title">Ta2n: Two-stage action alignment network for few-shot action recognition</div> <div class="author"> Shuyuan Li, Huabin Liu, Rui Qian, Yuxi Li, John See, Mengjuan Fei, Xiaoyuan Yu, and Weiyao Lin</div> <div class="periodical"> <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20029" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://github.com/R00Kie-Liu/TA2N" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right" data-doi="10.1609/aaai.v36i2.20029"></span> <span class="__dimensions_badge_embed__" data-doi="10.1609/aaai.v36i2.20029" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1609/aaai.v36i2.20029" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" target="_blank" rel="noopener noreferrer"></a> </div> <div class="abstract hidden"> <p>Few-shot action recognition aims to recognize novel action classes (query) using just a few samples (support). The majority of current approaches follow the metric learning paradigm, which learns to compare the similarity between videos. Recently, it has been observed that directly measuring this similarity is not ideal since different action instances may show distinctive temporal distribution, resulting in severe misalignment issues across query and support videos. In this paper, we arrest this problem from two distinct aspects–action duration misalignment and action evolution misalignment. We address them sequentially through a Two-stage Action Alignment Network (TA2N). The first stage locates the action by learning a temporal affine transform, which warps each video feature to its action duration while dismissing the action-irrelevant feature (eg background). Next, the second stage coordinates query feature to match the spatial-temporal action evolution of support by performing temporally rearrange and spatially offset prediction. Extensive experiments on benchmark datasets show the potential of the proposed method in achieving state-of-the-art performance for few-shot action recognition.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <img id="patch-tree-decoderjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/patch-tree-decoder.jpg"><div id="patch-tree-decoderjpg-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('patch-tree-decoderjpg-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="patch-tree-decoderjpg-modal-img"> </div> <script>var modal=document.getElementById("patch-tree-decoderjpg-modal"),img=document.getElementById("patch-tree-decoderjpg"),modalImg=document.getElementById("patch-tree-decoderjpg-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="fan2022speed" class="col-sm-8"> <div class="title">Speed up object detection on gigapixel-level images with patch arrangement</div> <div class="author"> Jiahao Fan, Huabin Liu, Wenjie Yang, John See, Aixin Zhang, and Weiyao Lin</div> <div class="periodical"> <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Fan_Speed_Up_Object_Detection_on_Gigapixel-Level_Images_With_Patch_Arrangement_CVPR_2022_paper.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right" data-doi="10.1109/CVPR52688.2022.00461"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/CVPR52688.2022.00461" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1109/CVPR52688.2022.00461" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" target="_blank" rel="noopener noreferrer"></a> </div> <div class="abstract hidden"> <p>With the appearance of super high-resolution (e.g., gigapixel-level) images, performing efficient object detection on such images becomes an important issue. Most existing works for efficient object detection on high-resolution images focus on generating local patches where objects may exist, and then every patch is detected independently. However, when the image resolution reaches gigapixel-level, they will suffer from a huge time cost for detecting numerous patches. Different from them, we devise a novel patch arrangement framework for fast object detection on gigapixel-level images. Under this framework, a Patch Arrangement Network (PAN) is proposed to accelerate the detection by determining which patches could be packed together into a compact canvas. Specifically, PAN consists of (1) a Patch Filter Module (PFM) (2) a Patch Packing Module (PPM). PFM filters patch candidates by learning to select patches between two granularities. Subsequently, from the remaining patches, PPM determines how to pack these patches together into a smaller number of canvases. Meanwhile, it generates an ideal layout of patches on canvas. These canvases are fed to the detector to get final results. Experiments show that our method could improve the inference speed on gigapixel-level images by 5 times while maintaining great performance.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <img id="skeletonGTpng" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/skeletonGT.png"><div id="skeletonGTpng-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('skeletonGTpng-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="skeletonGTpng-modal-img"> </div> <script>var modal=document.getElementById("skeletonGTpng-modal"),img=document.getElementById("skeletonGTpng"),modalImg=document.getElementById("skeletonGTpng-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="yang2023skeleton" class="col-sm-8"> <div class="title">Skeleton Ground Truth Extraction: Methodology, Annotation Tool and Benchmarks</div> <div class="author"> Cong Yang, Bipin Indurkhya, John See, Bo Gao, Yan Ke, Zeyd Boukhers, Zhenyu Yang, and Marcin Grzegorzek</div> <div class="periodical"> <em>International Journal of Computer Vision</em>, 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2310.06437" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://link.springer.com/article/10.1007/s11263-023-01926-3" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://researchportal.hw.ac.uk/files/104065760/s11263-023-01926-3.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/cong-yang/skeview" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right" data-doi="10.1007/s11263-023-01926-3"></span> <span class="__dimensions_badge_embed__" data-doi="10.1007/s11263-023-01926-3" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1007/s11263-023-01926-3" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" target="_blank" rel="noopener noreferrer"></a> </div> <div class="abstract hidden"> <p>Skeleton Ground Truth (GT) is critical to the success of supervised skeleton extraction methods, especially with the popularity of deep learning techniques. Furthermore, we see skeleton GTs used not only for training skeleton detectors with Convolutional Neural Networks (CNN), but also for evaluating skeleton-related pruning and matching algorithms. However, most existing shape and image datasets suffer from the lack of skeleton GT and inconsistency of GT standards. As a result, it is difficult to evaluate and reproduce CNN-based skeleton detectors and algorithms on a fair basis. In this paper, we present a heuristic strategy for object skeleton GT extraction in binary shapes and natural images. Our strategy is built on an extended theory of diagnosticity hypothesis, which enables encoding human-in-the-loop GT extraction based on clues from the target’s context, simplicity, and completeness. Using this strategy, we developed a tool, SkeView, to generate skeleton GT of 17 existing shape and image datasets. The GTs are then structurally evaluated with representative methods to build viable baselines for fair comparisons. Experiments demonstrate that GTs generated by our strategy yield promising quality with respect to standard consistency, and also provide a balance between simplicity and completeness.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <img id="ernetpng" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/ernet.png"><div id="ernetpng-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('ernetpng-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="ernetpng-modal-img"> </div> <script>var modal=document.getElementById("ernetpng-modal"),img=document.getElementById("ernetpng"),modalImg=document.getElementById("ernetpng-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="lim2023ernet" class="col-sm-8"> <div class="title">ERNet: An Efficient and Reliable Human-Object Interaction Detection Network</div> <div class="author"> JunYi Lim, Vishnu Monn Baskaran, Joanne Mun-Yee Lim, KokSheik Wong, John See, and Massimo Tistarelli</div> <div class="periodical"> <em>IEEE Transactions on Image Processing</em>, 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://github.com/Monash-CyPhi-AI-Research-Lab/ernet" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right" data-doi="10.1109/TIP.2022.3231528"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/TIP.2022.3231528" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1109/TIP.2022.3231528" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" target="_blank" rel="noopener noreferrer"></a> </div> <div class="abstract hidden"> <p>Human-Object Interaction (HOI) detection recognizes how persons interact with objects, which is advantageous in autonomous systems such as self-driving vehicles and collaborative robots. However, current HOI detectors are often plagued by model inefficiency and unreliability when making a prediction, which consequently limits its potential for real-world scenarios. In this paper, we address these challenges by proposing ERNet, an end-to-end trainable convolutional-transformer network for HOI detection. The proposed model employs an efficient multi-scale deformable attention to effectively capture vital HOI features. We also put forward a novel detection attention module to adaptively generate semantically rich instance and interaction tokens. These tokens undergo pre-emptive detections to produce initial region and vector proposals that also serve as queries which enhances the feature refinement process in the transformer decoders. Several impactful enhancements are also applied to improve the HOI representation learning. Additionally, we utilize a predictive uncertainty estimation framework in the instance and interaction classification heads to quantify the uncertainty behind each prediction. By doing so, we can accurately and reliably predict HOIs even under challenging scenarios. Experiment results on the HICO-Det, V-COCO, and HOI-A datasets demonstrate that the proposed model achieves state-of-the-art performance in detection accuracy and training efficiency. Codes are publicly available at <a href="https://github.com/Monash-CyPhi-AI-Research-Lab/ernet" target="_blank" rel="noopener noreferrer">https://github.com/Monash-CyPhi-AI-Research-Lab/ernet</a>.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <img id="blade-stitchingjpg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/blade-stitching.jpg"><div id="blade-stitchingjpg-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('blade-stitchingjpg-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="blade-stitchingjpg-modal-img"> </div> <script>var modal=document.getElementById("blade-stitchingjpg-modal"),img=document.getElementById("blade-stitchingjpg"),modalImg=document.getElementById("blade-stitchingjpg-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="yang2023towards" class="col-sm-8"> <div class="title">Towards accurate image stitching for drone-based wind turbine blade inspection</div> <div class="author"> Cong Yang, Xun Liu, Hua Zhou, Yan Ke, and John See</div> <div class="periodical"> <em>Renewable Energy</em>, 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.sciencedirect.com/science/article/pii/S0960148122018481" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://github.com/cong-yang/Blade30" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Dataset</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-badge-popover="right" data-doi="10.1016/j.renene.2022.12.063"></span> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.renene.2022.12.063" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 2px;"></span> <a class="plumx-plum-print-popup" href="https://plu.mx/plum/a/?doi=10.1016/j.renene.2022.12.063" data-hide-when-empty="true" data-size="small" data-popup="right" style="margin-bottom: 2px;" target="_blank" rel="noopener noreferrer"></a> </div> <div class="abstract hidden"> <p>Accurate image stitching is crucial to wind turbine blade visualization and defect analysis. It is inevitable that drone-captured images for blade inspection are high resolution and heavily overlapped. This also necessitates the stitching-based deduplication process on detected defects. However, the stitching task suffers from texture-poor blade surfaces, unstable drone pose (especially off-shore), and the lack of public blade datasets that cater to real-world challenges. In this paper, we present a simple yet efficient algorithm for robust and accurate blade image stitching. To promote further research, we also introduce a new dataset, Blade30, which contains 1,302 real drone-captured images covering 30 full blades captured under various conditions (both on- and off-shore), accompanied by a rich set of annotations such as defects and contaminations, etc. The proposed stitching algorithm generates the initial blade panorama based on blade edges and drone-blade distances at the coarse-grained level, followed by fine-grained adjustments optimized by regression-based texture and shape losses. Our method also fully utilizes the properties of blade images and prior information of the drone. Experiments report promising accuracy in blade stitching and defect deduplication tasks in the vision-based wind turbine blade inspection scenario, surpassing the performance of existing methods.</p> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%6A.%73%65%65@%68%77.%61%63.%75%6B" title="email"><i class="fas fa-envelope"></i></a> <a href="https://github.com/https://github.com/mudalab" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="/feed.xml" title="RSS Feed"><i class="fas fa-rss-square"></i></a> </div> <div class="contact-note"> You can even add a little note about which of these is the best way to reach you. </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 muda lab. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script async src="https://cdn.plu.mx/widget-popup.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>